
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Bingyu Jiang">
      
      
      
        <link rel="prev" href="../..">
      
      
        <link rel="next" href="../../NumericalPDE/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.5">
    
    
      
        <title>01. Introduction - byteyu</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../stylesheets/code.css">
    
      <link rel="stylesheet" href="../../stylesheets/math.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#01-introduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="byteyu" class="md-header__button md-logo" aria-label="byteyu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            byteyu
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              01. Introduction
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="byteyu" class="md-nav__button md-logo" aria-label="byteyu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    byteyu
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Machine Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
      
        

  

<nav class="md-nav md-nav--secondary" aria-label="Table of Contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of Contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#daily-quote" class="md-nav__link">
    <span class="md-ellipsis">
      Daily Quote
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tag" class="md-nav__link">
    <span class="md-ellipsis">
      Tag
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    <span class="md-ellipsis">
      Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      1. Linear Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Linear Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-assumptions" class="md-nav__link">
    <span class="md-ellipsis">
      Key Assumptions:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-multiple-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      2. Multiple Linear Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-logistic-regression-for-classification" class="md-nav__link">
    <span class="md-ellipsis">
      3. Logistic Regression (For Classification)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-polynomial-regression" class="md-nav__link">
    <span class="md-ellipsis">
      4. Polynomial Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-generalized-linear-model-glm" class="md-nav__link">
    <span class="md-ellipsis">
      5. Generalized Linear Model (GLM)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Generalized Linear Model (GLM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-form-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      General Form of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-glms" class="md-nav__link">
    <span class="md-ellipsis">
      Why GLMs?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-metrics-for-evaluating-regression-models" class="md-nav__link">
    <span class="md-ellipsis">
      Key Metrics for Evaluating Regression Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-dataset-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      A. Dataset Improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-improve-model-complexity-vc-dimension-bias-variance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vc-dimension-vapnik-chervonenkis-dimension" class="md-nav__link">
    <span class="md-ellipsis">
      VC Dimension (Vapnik-Chervonenkis Dimension)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-ways-to-measure-model-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      Other Ways to Measure Model Complexity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-regularization-avoid-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      C. Regularization (Avoid Overfitting)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C. Regularization (Avoid Overfitting)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regressionl2-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge regression(L2 regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution-bayesian-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution (Bayesian Interpretation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regressionl1-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Lasso Regression(L1 Regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_1" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-l1-regularization-perform-feature-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Why Does L1 Regularization Perform Feature Selection?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-net-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_2" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Intuition:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution_1" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-feature-engineering-selection" class="md-nav__link">
    <span class="md-ellipsis">
      D. Feature Engineering &amp; Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      E. Dimensionality Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-reduce-outliers-noise" class="md-nav__link">
    <span class="md-ellipsis">
      F. Reduce Outliers &amp; Noise
    </span>
  </a>
  
    <nav class="md-nav" aria-label="F. Reduce Outliers & Noise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intuition_3" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g-optimize-hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      G. Optimize Hyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="G. Optimize Hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grid-search" class="md-nav__link">
    <span class="md-ellipsis">
      Grid Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-search" class="md-nav__link">
    <span class="md-ellipsis">
      Random Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Optimization (BO)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Optimization (BO)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works_1" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of BO:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Components of BO:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-bayesian-optimization-is-powerful" class="md-nav__link">
    <span class="md-ellipsis">
      Why Bayesian Optimization is Powerful?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-of-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-ensemble-methods" class="md-nav__link">
    <span class="md-ellipsis">
      (6) Ensemble Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="(6) Ensemble Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Forest
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gbdt-gradient-boosted-decision-trees" class="md-nav__link">
    <span class="md-ellipsis">
      GBDT (Gradient Boosted Decision Trees)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GBDT (Gradient Boosted Decision Trees)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-by-step-process" class="md-nav__link">
    <span class="md-ellipsis">
      Step-by-Step Process:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-gbdt-works-well" class="md-nav__link">
    <span class="md-ellipsis">
      Why GBDT Works Well
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xgboost-extreme-gradient-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      XGBoost (Extreme Gradient Boosting)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lightgbm-light-gradient-boosting-machine" class="md-nav__link">
    <span class="md-ellipsis">
      LightGBM (Light Gradient Boosting Machine)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i-model-uncertainty-bayesianprobabilistic-models" class="md-nav__link">
    <span class="md-ellipsis">
      I. Model Uncertainty (Bayesian/Probabilistic Models)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Model Uncertainty (Bayesian/Probabilistic Models)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mle-for-beta" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \(\beta\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mle-for-sigma2" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \(\sigma^2\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computational-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Computational Techniques:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gaussian-processes" class="md-nav__link">
    <span class="md-ellipsis">
      Gaussian Processes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#j-optimization-algorithms-improving-model-training" class="md-nav__link">
    <span class="md-ellipsis">
      J. Optimization Algorithms (Improving Model Training)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="J. Optimization Algorithms (Improving Model Training)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      Newton's Method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum-based-update" class="md-nav__link">
    <span class="md-ellipsis">
      Momentum-based Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-model-architecture-optimization-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      K. Model Architecture Optimization (Deep Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l-interpretability-explainability" class="md-nav__link">
    <span class="md-ellipsis">
      L. Interpretability &amp; Explainability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../NumericalPDE/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Numerical PDEs
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Numerical PDEs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../HPC/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    High-Performance Computing
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            High-Performance Computing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  

<nav class="md-nav md-nav--secondary" aria-label="Table of Contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of Contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#daily-quote" class="md-nav__link">
    <span class="md-ellipsis">
      Daily Quote
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tag" class="md-nav__link">
    <span class="md-ellipsis">
      Tag
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    <span class="md-ellipsis">
      Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      1. Linear Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Linear Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-assumptions" class="md-nav__link">
    <span class="md-ellipsis">
      Key Assumptions:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-multiple-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      2. Multiple Linear Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-logistic-regression-for-classification" class="md-nav__link">
    <span class="md-ellipsis">
      3. Logistic Regression (For Classification)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-polynomial-regression" class="md-nav__link">
    <span class="md-ellipsis">
      4. Polynomial Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-generalized-linear-model-glm" class="md-nav__link">
    <span class="md-ellipsis">
      5. Generalized Linear Model (GLM)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Generalized Linear Model (GLM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-form-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      General Form of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-glms" class="md-nav__link">
    <span class="md-ellipsis">
      Why GLMs?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-metrics-for-evaluating-regression-models" class="md-nav__link">
    <span class="md-ellipsis">
      Key Metrics for Evaluating Regression Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-dataset-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      A. Dataset Improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-improve-model-complexity-vc-dimension-bias-variance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vc-dimension-vapnik-chervonenkis-dimension" class="md-nav__link">
    <span class="md-ellipsis">
      VC Dimension (Vapnik-Chervonenkis Dimension)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-ways-to-measure-model-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      Other Ways to Measure Model Complexity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-regularization-avoid-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      C. Regularization (Avoid Overfitting)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C. Regularization (Avoid Overfitting)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regressionl2-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge regression(L2 regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution-bayesian-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution (Bayesian Interpretation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regressionl1-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Lasso Regression(L1 Regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_1" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-l1-regularization-perform-feature-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Why Does L1 Regularization Perform Feature Selection?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-net-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_2" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Intuition:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution_1" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-feature-engineering-selection" class="md-nav__link">
    <span class="md-ellipsis">
      D. Feature Engineering &amp; Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      E. Dimensionality Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-reduce-outliers-noise" class="md-nav__link">
    <span class="md-ellipsis">
      F. Reduce Outliers &amp; Noise
    </span>
  </a>
  
    <nav class="md-nav" aria-label="F. Reduce Outliers & Noise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intuition_3" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g-optimize-hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      G. Optimize Hyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="G. Optimize Hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grid-search" class="md-nav__link">
    <span class="md-ellipsis">
      Grid Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-search" class="md-nav__link">
    <span class="md-ellipsis">
      Random Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Optimization (BO)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Optimization (BO)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works_1" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of BO:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Components of BO:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-bayesian-optimization-is-powerful" class="md-nav__link">
    <span class="md-ellipsis">
      Why Bayesian Optimization is Powerful?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-of-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-ensemble-methods" class="md-nav__link">
    <span class="md-ellipsis">
      (6) Ensemble Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="(6) Ensemble Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Forest
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gbdt-gradient-boosted-decision-trees" class="md-nav__link">
    <span class="md-ellipsis">
      GBDT (Gradient Boosted Decision Trees)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GBDT (Gradient Boosted Decision Trees)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-by-step-process" class="md-nav__link">
    <span class="md-ellipsis">
      Step-by-Step Process:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-gbdt-works-well" class="md-nav__link">
    <span class="md-ellipsis">
      Why GBDT Works Well
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xgboost-extreme-gradient-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      XGBoost (Extreme Gradient Boosting)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lightgbm-light-gradient-boosting-machine" class="md-nav__link">
    <span class="md-ellipsis">
      LightGBM (Light Gradient Boosting Machine)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i-model-uncertainty-bayesianprobabilistic-models" class="md-nav__link">
    <span class="md-ellipsis">
      I. Model Uncertainty (Bayesian/Probabilistic Models)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Model Uncertainty (Bayesian/Probabilistic Models)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mle-for-beta" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \(\beta\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mle-for-sigma2" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \(\sigma^2\)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computational-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Computational Techniques:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gaussian-processes" class="md-nav__link">
    <span class="md-ellipsis">
      Gaussian Processes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#j-optimization-algorithms-improving-model-training" class="md-nav__link">
    <span class="md-ellipsis">
      J. Optimization Algorithms (Improving Model Training)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="J. Optimization Algorithms (Improving Model Training)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      Newton's Method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum-based-update" class="md-nav__link">
    <span class="md-ellipsis">
      Momentum-based Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-model-architecture-optimization-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      K. Model Architecture Optimization (Deep Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l-interpretability-explainability" class="md-nav__link">
    <span class="md-ellipsis">
      L. Interpretability &amp; Explainability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="01-introduction">01. Introduction<a class="headerlink" href="#01-introduction" title="Permanent link">&para;</a></h1>
<h2 id="daily-quote">Daily Quote<a class="headerlink" href="#daily-quote" title="Permanent link">&para;</a></h2>
<blockquote>
<p>[!quote] When you are offended at any man's fault, turn to yourself and study your own failings. Then you will forget your anger.<br />
— Epictetus</p>
</blockquote>
<h2 id="tag">Tag<a class="headerlink" href="#tag" title="Permanent link">&para;</a></h2>
<hr />
<p>Machine learning (ML) is the <strong>scientific study</strong> of <strong>algorithms</strong> and <mark style="background: #FFB8EBA6;">statistical models</mark> that <mark style="background: #FF5582A6;">computer systems</mark> use to perform a specific task without using explicit instructions, relying on patterns and <mark style="background: #FFB86CA6;">inference</mark> instead. It is seen as a subset of <mark style="background: #BBFABBA6;">artificial intelligence</mark>. Machine learning algorithms build a <mark style="background: #ABF7F7A6;">mathematical model</mark> based on sample data, known as "<mark style="background: #ADCCFFA6;">training data</mark>", in order to make predictions or decisions without being explicitly programmed to perform the task.</p>
<p>The technique of using linear interpolation for tabulation was believed to be used by Babylonian astronomers and mathematicians in Seleucid Mesopotamia (last three centuries BC), and by the Greek astronomer and mathematician, Hipparchus (2nd century BC). A description of linear interpolation can be found in the Almagest (2nd century AD) by Ptolemy.<br />
<img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250218182124.png" /></p>
<hr />
<h2 id="regression">Regression<a class="headerlink" href="#regression" title="Permanent link">&para;</a></h2>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># first generate some random data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">npr</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">true_w</span><span class="p">,</span> <span class="n">true_b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="c1"># add some noise to the labels</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">*</span><span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">npr</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219150927.png" /></p>
<p>Regression is a fundamental concept in statistics and machine learning that models the relationship between a dependent variable (target) and one or more independent variables (features). It is widely used for <strong>prediction, inference, and understanding data trends</strong>.</p>
<hr />
<h3 id="1-linear-regression"><strong>1. Linear Regression</strong><a class="headerlink" href="#1-linear-regression" title="Permanent link">&para;</a></h3>
<p>Linear regression assumes a <strong>linear relationship</strong> between the input <span class="arithmatex">\(X\)</span> and output <span class="arithmatex">\(Y\)</span>. The model is expressed as:</p>
<div class="arithmatex">\[
Y = \beta_0 + \beta_1 X + \epsilon
\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(\beta_0\)</span> is the <strong>intercept</strong> (constant term).</p>
</li>
<li>
<p><span class="arithmatex">\(\beta_1\)</span> is the <strong>coefficient</strong> (slope).</p>
</li>
<li>
<p><span class="arithmatex">\(\epsilon\)</span> is the <strong>error term</strong> (captures noise or randomness in data).</p>
</li>
</ul>
<h4 id="key-assumptions"><strong>Key Assumptions:</strong><a class="headerlink" href="#key-assumptions" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Linearity:</strong> The relationship between <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> is linear.</p>
</li>
<li>
<p><strong>Independence:</strong> Observations are independent.</p>
</li>
<li>
<p><strong>Homoscedasticity:</strong> The variance of errors remains constant across values of <span class="arithmatex">\(X\)</span>.</p>
</li>
<li>
<p><strong>Normality of Errors:</strong> The residuals (errors) follow a normal distribution.</p>
</li>
</ol>
<h4 id="how-it-works"><strong>How It Works:</strong><a class="headerlink" href="#how-it-works" title="Permanent link">&para;</a></h4>
<p>Linear regression finds the <strong>best-fit line</strong> by minimizing the sum of squared errors, using <strong>Ordinary Least Squares (OLS)</strong>:</p>
<div class="arithmatex">\[
\min_{\beta} \sum (Y_i - \hat{Y}_i)^2
\]</div>
<p>where <span class="arithmatex">\(\hat{Y}_i\)</span> is the predicted value.</p>
<hr />
<h3 id="2-multiple-linear-regression"><strong>2. Multiple Linear Regression</strong><a class="headerlink" href="#2-multiple-linear-regression" title="Permanent link">&para;</a></h3>
<p>Extends linear regression to <strong>multiple features</strong>:</p>
<div class="arithmatex">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon
\]</div>
<p>It captures the influence of multiple factors on the target variable.</p>
<hr />
<h3 id="3-logistic-regression-for-classification"><strong>3. Logistic Regression (For Classification)</strong><a class="headerlink" href="#3-logistic-regression-for-classification" title="Permanent link">&para;</a></h3>
<p>Despite the name, <strong>logistic regression is used for classification</strong>, not regression. It models the probability of a binary outcome:</p>
<div class="arithmatex">\[
P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
\]</div>
<ul>
<li>
<p>It uses the <strong>logit (sigmoid) function</strong> to map predictions between 0 and 1.</p>
</li>
<li>
<p><strong>Generalized linear model (GLM)</strong>: It assumes a <strong>Bernoulli distribution</strong> for <span class="arithmatex">\(Y\)</span>.</p>
</li>
</ul>
<hr />
<h3 id="4-polynomial-regression"><strong>4. Polynomial Regression</strong><a class="headerlink" href="#4-polynomial-regression" title="Permanent link">&para;</a></h3>
<p>If a relationship is <strong>non-linear</strong>, we extend linear regression to include polynomial terms:</p>
<div class="arithmatex">\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_n X^n + \epsilon
\]</div>
<ul>
<li>
<p>It models curves instead of straight lines.</p>
</li>
<li>
<p>The higher the polynomial degree, the better it fits, but <strong>risk of overfitting</strong> increases.</p>
</li>
</ul>
<hr />
<h3 id="5-generalized-linear-model-glm"><strong>5. Generalized Linear Model (GLM)</strong><a class="headerlink" href="#5-generalized-linear-model-glm" title="Permanent link">&para;</a></h3>
<p>A <strong>Generalized Linear Model (GLM)</strong> extends ordinary linear regression by allowing for:</p>
<ol>
<li>
<p><strong>Different Distributions</strong>: The response variable <span class="arithmatex">\(Y\)</span> can follow different distributions (not just normal).</p>
</li>
<li>
<p><strong>Link Function</strong>: Instead of modeling <span class="arithmatex">\(Y\)</span> directly, GLMs use a function to relate the mean <span class="arithmatex">\(E[Y]\)</span> to the linear predictor.</p>
</li>
</ol>
<h4 id="general-form-of-glm"><strong>General Form of GLM</strong><a class="headerlink" href="#general-form-of-glm" title="Permanent link">&para;</a></h4>
<p>For a response variable <span class="arithmatex">\(Y\)</span>, GLMs assume:</p>
<div class="arithmatex">\[
g(E[Y]) = X\beta
\]</div>
<p>Where:</p>
<ul>
<li>
<p><span class="arithmatex">\(X\)</span> is the matrix of input features.</p>
</li>
<li>
<p><span class="arithmatex">\(\beta\)</span> are the coefficients to learn.</p>
</li>
<li>
<p><span class="arithmatex">\(g(\cdot)\)</span> is the <strong>link function</strong>, which connects the expected value of <span class="arithmatex">\(Y\)</span> to a linear combination of inputs.</p>
</li>
</ul>
<h4 id="key-components-of-glm"><strong>Key Components of GLM</strong><a class="headerlink" href="#key-components-of-glm" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Exponential Family of Distributions</strong>: <span class="arithmatex">\(Y\)</span> comes from a family of distributions (Normal, Poisson, Binomial, etc.).</p>
</li>
<li>
<p><strong>Link Function</strong> <span class="arithmatex">\(g(\cdot)\)</span>: Transforms <span class="arithmatex">\(E[Y]\)</span> into a linear model.</p>
</li>
<li>
<p><strong>Linear Predictor</strong>: <span class="arithmatex">\(X\beta\)</span> remains a linear combination of inputs.</p>
</li>
</ol>
<h4 id="why-glms"><strong>Why GLMs?</strong><a class="headerlink" href="#why-glms" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Generalizes <strong>linear regression</strong> to different distributions.</p>
</li>
<li>
<p>Uses <strong>Maximum Likelihood Estimation (MLE)</strong> instead of least squares.</p>
</li>
<li>
<p>More flexible than simple linear regression.</p>
</li>
</ul>
<hr />
<h3 id="key-metrics-for-evaluating-regression-models"><strong>Key Metrics for Evaluating Regression Models</strong><a class="headerlink" href="#key-metrics-for-evaluating-regression-models" title="Permanent link">&para;</a></h3>
<p>To measure the performance of a regression model, we use:</p>
<ol>
<li>
<p><strong>Mean Squared Error (MSE):</strong> Measures average squared differences between actual and predicted values.</p>
<div class="arithmatex">\[  
MSE = \frac{1}{n} \sum (Y_i - \hat{Y}_i)^2  
\]</div>
</li>
<li>
<p><strong>Root Mean Squared Error (RMSE):</strong> Square root of MSE, which brings errors to the same unit as the data.</p>
<div class="arithmatex">\[
RMSE = \sqrt{MSE}
\]</div>
</li>
<li>
<p><strong>Mean Absolute Error (MAE):</strong> Measures the average absolute differences.</p>
<div class="arithmatex">\[
MAE = \frac{1}{n} \sum |Y_i - \hat{Y}_i|
\]</div>
</li>
<li>
<p><strong><span class="arithmatex">\(R^2\)</span> (Coefficient of Determination):</strong> Explains the proportion of variance explained by the model.</p>
<div class="arithmatex">\[
R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
\]</div>
<hr />
</li>
</ol>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># the linear model</span>
<span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># gradient function</span>
<span class="n">loss_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">iterations</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.</span> <span class="c1"># initial values for the parameters</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
    <span class="c1"># compute gradient w.r.t model parameters</span>
    <span class="n">params_grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
    <span class="c1"># update parameters</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1">#print(loss_)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss: </span><span class="si">{}</span><span class="s2">, params </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss_</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>loss: 0.25392094254493713,<br />
params (Array(0.26151434, dtype=float32, weak_type=True), Array(1.8786527, dtype=float32, weak_type=True))</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219151017.png" /></p>
<h2 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link">&para;</a></h2>
<p>To <strong>reduce RMSE</strong>, we can use different strategies:</p>
<h3 id="a-dataset-improvements"><strong>A. Dataset Improvements</strong><a class="headerlink" href="#a-dataset-improvements" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>More Data</strong>: Collect additional samples to improve generalization.</p>
</li>
<li>
<p><strong>Data Augmentation</strong>: Create synthetic data using transformations (e.g., image rotations, SMOTE for imbalanced data).</p>
</li>
<li>
<p><strong>Synthetic Data Generation</strong>: GANs, Variational Autoencoders, or bootstrapping.</p>
</li>
<li>
<p><strong>Handling Missing Data</strong>: Imputation methods (mean, median, KNN imputation).</p>
</li>
<li>
<p><strong>Class Balancing</strong>: Oversampling, undersampling, or synthetic data generation (e.g., SMOTE for imbalanced datasets).</p>
</li>
</ul>
<h3 id="b-improve-model-complexity-vc-dimension-bias-variance-tradeoff"><strong>B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)</strong><a class="headerlink" href="#b-improve-model-complexity-vc-dimension-bias-variance-tradeoff" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Choose the Right Model</strong>: Simple models reduce overfitting; complex models improve accuracy but need regularization.</p>
</li>
<li>
<p><strong>Polynomial Features</strong>: Increase model complexity for non-linearity.</p>
</li>
<li>
<p><strong>Early Stopping</strong>: Stop training when validation loss starts increasing.</p>
</li>
</ul>
<h4 id="vc-dimension-vapnik-chervonenkis-dimension"><strong>VC Dimension (Vapnik-Chervonenkis Dimension)</strong><a class="headerlink" href="#vc-dimension-vapnik-chervonenkis-dimension" title="Permanent link">&para;</a></h4>
<p>VC dimension is a fundamental concept in statistical learning theory that measures the <strong>capacity</strong> or <strong>complexity</strong> of a model class. It quantifies how well a model can <strong>shatter</strong> (perfectly classify) different sets of data points.</p>
<blockquote>
<p>[!Definition]<br />
The <strong>VC dimension</strong> of a hypothesis class <span class="arithmatex">\(\mathcal{H}\)</span> is the largest number of points that can be <strong>shattered</strong> by some hypothesis in <span class="arithmatex">\(\mathcal{H}\)</span>.</p>
</blockquote>
<ul>
<li>
<p>A model <strong>shatters</strong> a dataset if it can classify all possible labelings (all <span class="arithmatex">\(2^n\)</span> assignments of class labels for <span class="arithmatex">\(n\)</span> points).</p>
</li>
<li>
<p>Higher VC dimension → More complex model → Higher capacity to fit data (risk of overfitting).</p>
</li>
<li>
<p>Example:</p>
<ul>
<li>
<p>A linear classifier in 2D can <strong>shatter</strong> at most <strong>3</strong> points, so its VC dimension is 3.</p>
</li>
<li>
<p>A linear classifier in <span class="arithmatex">\(d\)</span>-dimensions has VC dimension <span class="arithmatex">\(d+1\)</span>.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Implications:</strong></p>
<ul>
<li>
<p><strong>Lower VC dimension</strong> → Model is simple (underfitting risk).</p>
</li>
<li>
<p><strong>Higher VC dimension</strong> → Model is complex (overfitting risk).</p>
</li>
<li>
<p><strong>VC Theorem</strong>: If a model has a finite VC dimension <span class="arithmatex">\(d\)</span>, it generalizes well if the number of training examples <span class="arithmatex">\(n\)</span> is sufficiently larger than <span class="arithmatex">\(d\)</span>.</p>
</li>
</ul>
<hr />
<h4 id="other-ways-to-measure-model-complexity"><strong>Other Ways to Measure Model Complexity</strong><a class="headerlink" href="#other-ways-to-measure-model-complexity" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Rademacher Complexity</strong></p>
<ul>
<li>
<p>Measures how well a hypothesis class can fit random noise.</p>
</li>
<li>
<p>If a model class has high Rademacher complexity, it can overfit random labels.</p>
</li>
</ul>
</li>
<li>
<p><strong>PAC-Bayes</strong>, </p>
</li>
<li>
<p><strong>margin-based bounds</strong>  </p>
</li>
<li>
<p><strong>Geometric way</strong></p>
</li>
</ol>
<hr />
<ul>
<li>
<p>If the model is <strong>too simple</strong>, it underfits. We can:</p>
</li>
<li>
<p>Use polynomial regression instead of linear.</p>
</li>
<li>
<p>Add interaction terms.</p>
</li>
<li>
<p>Use <strong>nonlinear models</strong> (e.g., random forests, neural networks).
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># the nonlinear model</span>
<span class="k">def</span> <span class="nf">nonlinear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="n">idx</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">nonlinear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># gradient function</span>
<span class="n">loss_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">order</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span> 

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="n">params_grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="c1"># update parameters</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">):</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Order: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">loss_</span><span class="p">))</span>
</code></pre></div></td></tr></table></div></p>
</li>
</ul>
<blockquote>
<p>Order: 2, loss: 0.25392<br />
Order: 4, loss: 0.24112<br />
Order: 6, loss: 0.19777<br />
Order: 8, loss: 0.16633<br />
Order: 10, loss: 0.15277<br />
Order: 12, loss: 0.15003<br />
Order: 14, loss: 0.15189</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154334.png" /></p>
<h3 id="c-regularization-avoid-overfitting"><strong>C. Regularization (Avoid Overfitting)</strong><a class="headerlink" href="#c-regularization-avoid-overfitting" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>L1 Regularization (Lasso <span class="arithmatex">\(\ell_1\)</span>)</strong>: Encourages sparsity (feature selection).</p>
</li>
<li>
<p><strong>L2 Regularization (<span class="arithmatex">\(\ell_2\)</span> Ridge)</strong>: Reduces large weights, preventing overfitting.</p>
</li>
<li>
<p><strong>Elastic Net</strong>: Combination of L1 and L2.</p>
</li>
<li>
<p><strong>Dropout (Neural Networks)</strong>: Randomly removes neurons during training.</p>
</li>
<li>
<p><strong>Batch Normalization</strong>: Normalizes activations in deep networks.</p>
</li>
</ul>
<hr />
<h4 id="ridge-regressionl2-regulation">Ridge regression(L2 regulation)<a class="headerlink" href="#ridge-regressionl2-regulation" title="Permanent link">&para;</a></h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># encoder</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># ridge regression</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.04097932204604149</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154350.png" /></p>
<p><strong>Ridge regression</strong> is a type of <strong>linear regression</strong> that includes an <span class="arithmatex">\(L_2\)</span>-norm penalty to prevent overfitting. The objective function is:</p>
<div class="arithmatex">\[
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(y_i\)</span> are the observed values,</p>
</li>
<li>
<p><span class="arithmatex">\(X_i\)</span> are the input features,</p>
</li>
<li>
<p><span class="arithmatex">\(\beta\)</span> are the regression coefficients,</p>
</li>
<li>
<p><span class="arithmatex">\(\lambda\)</span> is the <strong>regularization parameter</strong>, which controls the trade-off between fitting the data and keeping coefficients small.</p>
</li>
</ul>
<h4 id="intuition"><strong>Intuition:</strong><a class="headerlink" href="#intuition" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Adding <span class="arithmatex">\(\lambda \sum \beta_j^2\)</span> <strong>shrinks</strong> the coefficients <span class="arithmatex">\(\beta_j\)</span> towards zero but <strong>never exactly zero</strong>.</p>
</li>
<li>
<p>Ridge regression helps when features are <strong>highly correlated</strong>, reducing variance and preventing overfitting.</p>
</li>
</ul>
<h4 id="relation-to-prior-distribution-bayesian-interpretation"><strong>Relation to Prior Distribution (Bayesian Interpretation)</strong><a class="headerlink" href="#relation-to-prior-distribution-bayesian-interpretation" title="Permanent link">&para;</a></h4>
<p>Ridge regression can be seen as <strong>Bayesian linear regression</strong> with a <strong>Gaussian prior</strong> on the coefficients:</p>
<p>$$</p>
<p>\beta_j \sim \mathcal{N}(0, \tau^2)</p>
<p>$$</p>
<p>where <span class="arithmatex">\(\tau^2 = \frac{1}{\lambda}\)</span>. This means:</p>
<ul>
<li>
<p>The model assumes that <strong>larger coefficients are less likely</strong>, which enforces shrinkage.</p>
</li>
<li>
<p>A <strong>stronger prior (larger <span class="arithmatex">\(\lambda\)</span>)</strong> shrinks the coefficients more.</p>
</li>
</ul>
<hr />
<h4 id="lasso-regressionl1-regulation">Lasso Regression(L1 Regulation)<a class="headerlink" href="#lasso-regressionl1-regulation" title="Permanent link">&para;</a></h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Loss): </span><span class="si">{</span><span class="n">mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.043433766812086105</p>
</blockquote>
<p><strong>Lasso (Least Absolute Shrinkage and Selection Operator)</strong> is another <strong>regularized regression</strong> method, but it uses an <span class="arithmatex">\(L_1\)</span>-norm penalty instead of <span class="arithmatex">\(L_2\)</span>:</p>
<div class="arithmatex">\[
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\]</div>
<h4 id="intuition_1"><strong>Intuition:</strong><a class="headerlink" href="#intuition_1" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>The <strong>absolute value</strong> penalty forces some coefficients to be exactly <strong>zero</strong>, effectively performing <strong>feature selection</strong>.</p>
</li>
<li>
<p>Unlike Ridge regression, Lasso can create <strong>sparse models</strong>, making it useful when there are many irrelevant features.</p>
</li>
</ul>
<h4 id="relation-to-prior-distribution"><strong>Relation to Prior Distribution</strong><a class="headerlink" href="#relation-to-prior-distribution" title="Permanent link">&para;</a></h4>
<p>Lasso regression corresponds to <strong>Bayesian regression</strong> with a <strong>Laplace prior</strong>:</p>
<div class="arithmatex">\[
\beta_j \sim \text{Laplace}(0, b)
\]</div>
<p>where <span class="arithmatex">\(b = \frac{1}{\lambda}\)</span>. The <strong>Laplace distribution</strong> is sharply peaked at zero, which encourages sparsity. This is why Lasso forces some coefficients to be exactly <strong>zero</strong>.</p>
<h4 id="why-does-l1-regularization-perform-feature-selection"><strong>Why Does L1 Regularization Perform Feature Selection?</strong><a class="headerlink" href="#why-does-l1-regularization-perform-feature-selection" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Sparsity Effect:</strong></p>
<ul>
<li>
<p>The L1 penalty encourages some coefficients <span class="arithmatex">\(\beta_j\)</span> to be exactly <strong>zero</strong>.</p>
</li>
<li>
<p>This leads to a <strong>sparse</strong> model, where only a subset of the features are retained.</p>
</li>
<li>
<p>Features with zero coefficients are effectively removed from the model.</p>
</li>
</ul>
</li>
<li>
<p><strong>Optimization Property:</strong></p>
<ul>
<li>
<p>The L1 norm creates a <strong>non-differentiable</strong> point at <span class="arithmatex">\(\beta_j = 0\)</span>, which forces some coefficients to shrink to zero.</p>
</li>
<li>
<p>Geometrically, the constraint region forms a <strong>diamond shape</strong>, which makes it more likely for the optimal solution to lie on the axes (i.e., some coefficients are zero).</p>
</li>
</ul>
</li>
<li>
<p><strong>Automatic Feature Selection:</strong></p>
<ul>
<li>
<p>Unlike L2 regularization (<strong>Ridge regression</strong>), which only shrinks coefficients towards zero, L1 <strong>eliminates</strong> irrelevant features entirely.</p>
</li>
<li>
<p>This is useful when you have <strong>high-dimensional data</strong> with many irrelevant or redundant features.</p>
</li>
</ul>
</li>
</ol>
<hr />
<h3 id="elastic-net-regression">Elastic Net Regression<a class="headerlink" href="#elastic-net-regression" title="Permanent link">&para;</a></h3>
<p><strong>Elastic Net</strong> combines both Ridge (<span class="arithmatex">\(L_2\)</span>) and Lasso (<span class="arithmatex">\(L_1\)</span>) penalties:</p>
<div class="arithmatex">\[
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(\lambda_1\)</span> controls the Lasso part (feature selection).</p>
</li>
<li>
<p><span class="arithmatex">\(\lambda_2\)</span> controls the Ridge part (shrinkage and handling collinearity).</p>
</li>
</ul>
<h3 id="intuition_2"><strong>Intuition:</strong><a class="headerlink" href="#intuition_2" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>If features are <strong>highly correlated</strong>, Lasso can randomly pick one and discard the others. Elastic Net avoids this issue by keeping a mix of features.</p>
</li>
<li>
<p>It <strong>selects features like Lasso</strong> while <strong>shrinking coefficients like Ridge</strong>.</p>
</li>
</ul>
<h4 id="relation-to-prior-distribution_1"><strong>Relation to Prior Distribution</strong><a class="headerlink" href="#relation-to-prior-distribution_1" title="Permanent link">&para;</a></h4>
<p>Elastic Net corresponds to a <strong>Mixture Prior</strong>:</p>
<ul>
<li>
<p>It assumes a <strong>combination</strong> of <strong>Gaussian (Ridge) and Laplace (Lasso) priors</strong> on the coefficients.</p>
</li>
<li>
<p>This means that some coefficients get <strong>shrunk</strong> (like Ridge), while others get <strong>sparsified</strong> (like Lasso).</p>
</li>
</ul>
<hr />
<h3 id="d-feature-engineering-selection"><strong>D. Feature Engineering &amp; Selection</strong><a class="headerlink" href="#d-feature-engineering-selection" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>One-hot Encoding</strong>: Convert categorical variables into numerical.</p>
</li>
<li>
<p><strong>Standardization (Z-score, Min-Max Scaling)</strong>: Normalize feature values.</p>
</li>
<li>
<p>Other <strong>feature transformations</strong> (log, power, etc.).</p>
</li>
<li>
<p><strong>Feature Selection Methods</strong>:</p>
<ul>
<li>
<p><strong>Univariate Tests</strong>: Select top-ranked features based on correlation.</p>
</li>
<li>
<p><strong>Recursive Feature Elimination (RFE)</strong>: Iteratively remove less important features.</p>
</li>
<li><strong>Lasso (L1 Regularization)</strong>: Selects only important features.</li>
<li>Remove <strong>irrelevant or redundant features</strong> to reduce noise.</li>
</ul>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>

<span class="c1"># omit repeated code</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">))),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>loss: 0.07207518070936203</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154941.png" /></p>
<ul>
<li>Perform <strong>dimensionality reduction</strong> (PCA, autoencoders).</li>
</ul>
<h3 id="e-dimensionality-reduction"><strong>E. Dimensionality Reduction</strong><a class="headerlink" href="#e-dimensionality-reduction" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>PCA (Principal Component Analysis)</strong>: Reduces correlated features.</p>
</li>
<li>
<p><strong>LDA (Linear Discriminant Analysis)</strong>: Used in classification tasks.</p>
</li>
<li>
<p><strong>t-SNE &amp; UMAP</strong>: Nonlinear dimensionality reduction for visualization.</p>
</li>
</ul>
<h3 id="f-reduce-outliers-noise"><strong>F. Reduce Outliers &amp; Noise</strong><a class="headerlink" href="#f-reduce-outliers-noise" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Z-score, IQR Method</strong>: Remove extreme outliers.</p>
</li>
<li>
<p><strong>Robust Loss Functions</strong>: Huber loss, quantile loss.</p>
</li>
<li>
<p><strong>Denoising Methods</strong>: Gaussian smoothing, moving average.</p>
<ul>
<li>RMSE is sensitive to <strong>outliers</strong>. </li>
</ul>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>

<span class="n">iso_forest</span> <span class="o">=</span> <span class="n">IsolationForest</span><span class="p">(</span><span class="n">contamination</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>  
<span class="n">outliers</span> <span class="o">=</span> <span class="n">iso_forest</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">inliers</span> <span class="o">=</span> <span class="n">outliers</span> <span class="o">==</span> <span class="mi">1</span>

<span class="n">X_filtered</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inliers</span><span class="p">]</span>
<span class="n">Y_filtered</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">inliers</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.11134050786495209</p>
</blockquote>
<ul>
<li>Robust regression (Huber loss).
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">HuberRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
</code></pre></div></td></tr></table></div><blockquote>
<p>Mean Squared Error (Loss): 0.03720296174287796</p>
</blockquote>
</li>
</ul>
<p>The <strong>Huber loss</strong> is a combination of <strong>squared loss</strong> (used in ordinary least squares) and <strong>absolute loss</strong> (used in robust regression), designed to handle <strong>outliers</strong> more gracefully. It is particularly useful when you want a model that is robust to <strong>outliers</strong> but still penalizes small errors quadratically, like the regular least squares.</p>
<blockquote>
<p>[!Definition] Definition<br />
The Huber loss function <span class="arithmatex">\(L_\delta(y, \hat{y})\)</span> is defined as:
$$
L_\delta(y, \hat{y}) =<br />
\begin{cases}<br />
\frac{1}{2}(y - \hat{y})^2, &amp; \text{for} \ |y - \hat{y}| \leq \delta \<br />
\delta |y - \hat{y}| - \frac{1}{2} \delta^2, &amp; \text{for} \ |y - \hat{y}| &gt; \delta<br />
\end{cases}
$$
where:
- <span class="arithmatex">\(y\)</span> is the true value,
- <span class="arithmatex">\(\hat{y}\)</span> is the predicted value,
- <span class="arithmatex">\(\delta\)</span> is a threshold that controls the transition between quadratic and linear loss.</p>
</blockquote>
<h4 id="intuition_3"><strong>Intuition:</strong><a class="headerlink" href="#intuition_3" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>For errors less than or equal to <span class="arithmatex">\(\delta\)</span>, the loss behaves like a <strong>squared error</strong> (quadratic), which heavily penalizes small deviations.</p>
</li>
<li>
<p>For large errors (i.e., when the error is greater than <span class="arithmatex">\(\delta\)</span>), the loss behaves like <strong>absolute error</strong>, which grows linearly, avoiding excessive penalties for large deviations or outliers.</p>
</li>
</ul>
<p>This makes the <strong>Huber loss</strong> more <strong>robust to outliers</strong> compared to squared error (which can be highly influenced by large deviations).</p>
<h3 id="g-optimize-hyperparameters"><strong>G. Optimize Hyperparameters</strong><a class="headerlink" href="#g-optimize-hyperparameters" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Grid Search</strong>: Exhaustively tries all combinations.</p>
</li>
<li>
<p><strong>Random Search</strong>: Randomly samples hyperparameters.</p>
</li>
<li>
<p><strong>Bayesian Optimization</strong>: Uses probability to find optimal hyperparameters.</p>
</li>
<li>
<p><strong>Evolutionary Algorithms</strong>: Genetic algorithms to find the best configuration.</p>
</li>
</ul>
<h4 id="grid-search"><strong>Grid Search</strong><a class="headerlink" href="#grid-search" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Exhaustively searches over a predefined set of hyperparameters.</p>
</li>
<li>
<p>Creates a <strong>grid</strong> of possible values and evaluates each combination.</p>
</li>
</ul>
<p><strong>Example:</strong></p>
<p>If we have:</p>
<ul>
<li>
<p>Learning rate <span class="arithmatex">\(\eta\)</span> in {0.001, 0.01, 0.1}</p>
</li>
<li>
<p>Regularization <span class="arithmatex">\(\lambda\)</span> in {0.001, 0.01, 0.1}</p>
</li>
</ul>
<p>Grid search tests all <span class="arithmatex">\(3 \times 3 = 9\)</span> combinations.</p>
<p>✅ <strong>Pros:</strong></p>
<ul>
<li>
<p>Guarantees best result within search space.</p>
</li>
<li>
<p>Simple and systematic.</p>
</li>
</ul>
<p>❌ <strong>Cons:</strong></p>
<ul>
<li>
<p>Computationally <strong>expensive</strong>.</p>
</li>
<li>
<p>Wasteful if only a few hyperparameters matter.</p>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best score:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h4 id="random-search"><strong>Random Search</strong><a class="headerlink" href="#random-search" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Randomly samples hyperparameter values from a distribution.</p>
</li>
<li>
<p>Does not evaluate all combinations but instead explores the space <strong>stochastically</strong>.</p>
</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>Instead of testing all 9 combinations, randomly select 5 combinations.</li>
</ul>
<p>✅ <strong>Pros:</strong></p>
<ul>
<li>
<p>Often more <strong>efficient</strong> than grid search.</p>
</li>
<li>
<p>Works well when <strong>only a few hyperparameters significantly affect performance</strong>.</p>
</li>
<li>
<p><strong>More effective in high-dimensional spaces</strong>.</p>
</li>
</ul>
<p>❌ <strong>Cons:</strong>
- No guarantee of finding the absolute best hyperparameters.</p>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_dist</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:&quot;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best score:&quot;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h4 id="bayesian-optimization-bo"><strong>Bayesian Optimization (BO)</strong><a class="headerlink" href="#bayesian-optimization-bo" title="Permanent link">&para;</a></h4>
<p>Grid and random search <strong>do not use past evaluations</strong> to guide future searches. <strong>Bayesian Optimization (BO)</strong> does.</p>
<h5 id="how-it-works_1"><strong>How It Works</strong><a class="headerlink" href="#how-it-works_1" title="Permanent link">&para;</a></h5>
<ol>
<li>
<p>Define a <strong>prior</strong> belief about the function mapping hyperparameters to performance.</p>
</li>
<li>
<p>Use a <strong>surrogate model</strong> (usually a <strong>Gaussian Process</strong>) to estimate the objective function.</p>
</li>
<li>
<p>Select the next hyperparameter set based on an <strong>acquisition function</strong> (e.g., Expected Improvement, Upper Confidence Bound).</p>
</li>
<li>
<p>Evaluate the model with these hyperparameters.</p>
</li>
<li>
<p>Update the belief and repeat.</p>
</li>
</ol>
<hr />
<h4 id="key-components-of-bo"><strong>Key Components of BO:</strong><a class="headerlink" href="#key-components-of-bo" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Surrogate Model:</strong>  </p>
<ul>
<li>
<p>Typically a <strong>Gaussian Process (GP)</strong> that models the unknown function.</p>
</li>
<li>
<p>Uses previous evaluations to estimate performance at new points.</p>
</li>
</ul>
</li>
<li>
<p><strong>Acquisition Function:</strong> </p>
<ul>
<li>Guides the search by balancing <strong>exploration</strong> (trying new areas) and <strong>exploitation</strong> (focusing on promising regions).</li>
</ul>
</li>
<li>
<p>Common choices:   </p>
<ul>
<li>
<p><strong>Expected Improvement (EI)</strong>: Pick points expected to improve the best result.</p>
</li>
<li>
<p><strong>Upper Confidence Bound (UCB)</strong>: Pick points with high uncertainty.</p>
</li>
</ul>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">bayes_opt</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>

<span class="k">def</span> <span class="nf">rf_cv</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">),</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">max_depth</span><span class="p">),</span>
        <span class="n">min_samples_split</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">min_samples_split</span><span class="p">),</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  <span class="c1"># Optimization objective</span>

<span class="c1"># Define hyperparameter space</span>
<span class="n">pbounds</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span>
    <span class="n">f</span><span class="o">=</span><span class="n">rf_cv</span><span class="p">,</span>
    <span class="n">pbounds</span><span class="o">=</span><span class="n">pbounds</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">maximize</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found:&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h5 id="why-bayesian-optimization-is-powerful"><strong>Why Bayesian Optimization is Powerful?</strong><a class="headerlink" href="#why-bayesian-optimization-is-powerful" title="Permanent link">&para;</a></h5>
<p>✅ <strong>More efficient than grid/random search</strong> → Finds good hyperparameters in fewer evaluations.</p>
<p>✅ <strong>Works well for expensive models</strong> (e.g., deep learning, reinforcement learning).</p>
<p>✅ <strong>Uses prior knowledge</strong> to guide search intelligently.</p>
<p>❌ <strong>Downside?</strong></p>
<ul>
<li>
<p>Computationally <strong>expensive</strong> for very high-dimensional problems.</p>
</li>
<li>
<p>Assumes a smooth objective function, which may not always hold.</p>
</li>
</ul>
<hr />
<h4 id="comparison-of-methods"><strong>Comparison of Methods</strong><a class="headerlink" href="#comparison-of-methods" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Grid Search</strong></td>
<td>Systematic, finds best result within search space</td>
<td>Expensive, infeasible in high dimensions</td>
</tr>
<tr>
<td><strong>Random Search</strong></td>
<td>More efficient than grid search, better exploration</td>
<td>No guarantee of best hyperparameters</td>
</tr>
<tr>
<td><strong>Bayesian Optimization</strong></td>
<td>Uses prior knowledge, efficient in low-data settings</td>
<td>Computationally expensive in high dimensions</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="6-ensemble-methods"><strong>(6) Ensemble Methods</strong><a class="headerlink" href="#6-ensemble-methods" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Bagging</strong> (Bootstrap Aggregating): Train multiple models on bootstrapped datasets (e.g., Random Forest).</p>
</li>
<li>
<p><strong>Boosting</strong>: Sequential training that corrects previous errors (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM).</p>
</li>
<li>
<p><strong>Stacking</strong>: Combine multiple models via a meta-model.</p>
</li>
</ul>
<h4 id="random-forest">Random Forest<a class="headerlink" href="#random-forest" title="Permanent link">&para;</a></h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.008086484856903553</p>
</blockquote>
<p><img alt="" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219172219.png" /></p>
<p>Random Forest is an <strong>ensemble learning method</strong> that builds multiple decision trees and combines their outputs to improve prediction accuracy and reduce overfitting.</p>
<p><strong>How Random Forest Works</strong></p>
<ul>
<li>
<p><strong>Bootstrap Aggregation (Bagging):</strong></p>
</li>
<li>
<p>Random Forest trains multiple decision trees on different <strong>bootstrapped samples</strong> of the dataset.</p>
</li>
<li>
<p>Each tree is trained on a <strong>random subset</strong> (without replacement) of the features.</p>
</li>
<li>
<p>The predictions from all trees are <strong>averaged</strong> (for regression) or use <strong>majority voting</strong> (for classification).</p>
</li>
</ul>
<p><strong>Why it Works:</strong></p>
<ul>
<li>
<p>Reduces <strong>overfitting</strong> (compared to a single decision tree).</p>
</li>
<li>
<p>Handles <strong>high-dimensional data</strong> well.</p>
</li>
<li>
<p><strong>Robust to noise</strong> and missing values.</p>
</li>
</ul>
<p><strong>Pros &amp; Cons</strong></p>
<p>✅ Reduces variance (compared to individual trees).</p>
<p>✅ Handles categorical &amp; numerical features well.</p>
<p>✅ Works well for large datasets.</p>
<p>❌ Slower than a single decision tree.</p>
<p>❌ Large ensembles can be computationally expensive.</p>
<h4 id="gbdt-gradient-boosted-decision-trees">GBDT (Gradient Boosted Decision Trees)<a class="headerlink" href="#gbdt-gradient-boosted-decision-trees" title="Permanent link">&para;</a></h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>

<span class="n">lgb_train</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;regression&#39;</span><span class="p">,</span> <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gbdt&#39;</span><span class="p">,</span> <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span>
<span class="n">lgb_model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lgb_train</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">lgb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.19826513528823853</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219172346.png" /></p>
<p>GBDT (Gradient Boosted Decision Trees) is a <strong>powerful ensemble learning method</strong> that builds multiple decision trees sequentially to <strong>minimize a given loss function</strong> using gradient descent. It is widely used in machine learning for regression, classification, and ranking problems.</p>
<p><strong>How GBDT Works</strong></p>
<p>GBDT is based on the <strong>boosting</strong> technique, where models are trained <strong>sequentially</strong>, and each tree tries to correct the errors made by the previous ones.</p>
<h5 id="step-by-step-process"><strong>Step-by-Step Process:</strong><a class="headerlink" href="#step-by-step-process" title="Permanent link">&para;</a></h5>
<ol>
<li>
<p><strong>Start with a weak model</strong></p>
<ul>
<li>
<p>Usually, this is a simple decision tree (often a shallow one, also called a weak learner).</p>
</li>
<li>
<p>The first tree predicts the target values roughly.</p>
</li>
</ul>
</li>
<li>
<p><strong>Compute the residual errors (gradients of the loss function)</strong></p>
<ul>
<li>For regression:</li>
</ul>
<div class="arithmatex">\[
r_i = y_i - \hat{y}_i
\]</div>
<ul>
<li>For classification:</li>
</ul>
<div class="arithmatex">\[
r_i = -\frac{\partial L(y, \hat{y})}{\partial \hat{y}}
\]</div>
<ul>
<li>The model learns the difference (residuals) between the predicted and actual values.</li>
</ul>
</li>
<li>
<p><strong>Train a new decision tree to predict the residuals</strong></p>
<ul>
<li>This new tree focuses on correcting the errors made by the previous one.</li>
</ul>
</li>
<li>
<p><strong>Update the prediction</strong></p>
<ul>
<li>Add the new tree's predictions to the overall model:</li>
</ul>
<div class="arithmatex">\[
F_{m+1}(x) = F_m(x) + \eta h_m(x)
\]</div>
<ul>
<li><span class="arithmatex">\(\eta\)</span> is the <strong>learning rate</strong>, which controls how much each tree contributes to the final prediction.</li>
</ul>
</li>
<li>
<p><strong>Repeat the process</strong></p>
<ul>
<li>Keep adding new trees until a stopping criterion is met (e.g., a fixed number of trees or no significant improvement in performance).</li>
</ul>
</li>
</ol>
<h5 id="why-gbdt-works-well"><strong>Why GBDT Works Well</strong><a class="headerlink" href="#why-gbdt-works-well" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p><strong>Handles Non-Linearity:</strong> Unlike linear models, GBDT captures complex, non-linear relationships.</p>
</li>
<li>
<p><strong>Adaptive Learning:</strong> Each new tree corrects errors made by previous ones.</p>
</li>
<li>
<p><strong>Robust to Outliers:</strong> GBDT can be robust if loss functions like Huber loss are used.</p>
</li>
<li>
<p><strong>Feature Importance:</strong> It naturally provides feature importance rankings.</p>
</li>
</ul>
<hr />
<h4 id="xgboost-extreme-gradient-boosting"><strong>XGBoost (Extreme Gradient Boosting)</strong><a class="headerlink" href="#xgboost-extreme-gradient-boosting" title="Permanent link">&para;</a></h4>
<p>XGBoost is an optimized implementation of <strong>gradient boosting</strong> that is highly efficient and widely used in machine learning competitions.</p>
<p><strong>How XGBoost Works</strong></p>
<ul>
<li>
<p><strong>Gradient Boosting:</strong></p>
<ul>
<li>
<p>Unlike Random Forest (which trains trees independently), XGBoost <strong>trains trees sequentially</strong>, where each new tree corrects the errors of the previous ones.</p>
</li>
<li>
<p>It <strong>minimizes a loss function</strong> using <strong>gradient descent</strong>, hence the name "gradient boosting."</p>
</li>
</ul>
</li>
<li>
<p><strong>Key Optimizations:</strong></p>
<ul>
<li>
<p><strong>Regularization:</strong> Adds L1/L2 penalties (like Lasso/Ridge) to prevent overfitting.</p>
</li>
<li>
<p><strong>Tree Pruning:</strong> Uses a <strong>maximum depth</strong> instead of splitting until pure.</p>
</li>
<li>
<p><strong>Handling Missing Values:</strong> Automatically learns best imputation.</p>
</li>
<li>
<p><strong>Parallelization:</strong> Efficiently builds trees using multi-threading.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Pros &amp; Cons</strong></p>
<p>✅ Faster than traditional gradient boosting (due to optimizations).</p>
<p>✅ Handles large-scale data efficiently.</p>
<p>✅ Highly customizable with tunable hyperparameters.</p>
<p>❌ Can be prone to overfitting if not tuned properly.</p>
<p>❌ Sensitive to hyperparameter settings.</p>
<hr />
<h4 id="lightgbm-light-gradient-boosting-machine"><strong>LightGBM (Light Gradient Boosting Machine)</strong><a class="headerlink" href="#lightgbm-light-gradient-boosting-machine" title="Permanent link">&para;</a></h4>
<p>LightGBM is another gradient boosting library, optimized for <strong>speed</strong> and <strong>efficiency</strong>.</p>
<p><strong>How LightGBM Works</strong></p>
<ul>
<li>
<p><strong>Key Differences from XGBoost:</strong></p>
</li>
<li>
<p><strong>Leaf-wise tree growth:</strong></p>
<ul>
<li>
<p>XGBoost grows trees level-wise (uniform expansion).</p>
</li>
<li>
<p>LightGBM grows trees leaf-wise (expands the most promising leaf first), leading to deeper trees and faster convergence.</p>
</li>
</ul>
</li>
<li>
<p><strong>Histogram-based splitting:</strong></p>
<ul>
<li>Instead of checking every data point for splits, LightGBM groups values into <strong>histogram bins</strong>, reducing computation.</li>
</ul>
</li>
<li>
<p><strong>Better memory efficiency:</strong></p>
<ul>
<li>Uses <strong>less RAM</strong> than XGBoost due to histogram-based processing.</li>
</ul>
</li>
</ul>
<p><strong>Pros &amp; Cons</strong></p>
<p>✅ Much <strong>faster</strong> than XGBoost, especially for large datasets.</p>
<p>✅ <strong>Handles categorical features</strong> without needing one-hot encoding.</p>
<p>✅ Scales well to large datasets.</p>
<p>❌ May be <strong>less interpretable</strong> than XGBoost.</p>
<p>❌ Can overfit if not properly tuned.</p>
<p><strong>When to Use Which?</strong></p>
<ul>
<li>
<p><strong>Random Forest</strong> → When you need an <strong>interpretable model</strong> and have <strong>limited data</strong>.</p>
</li>
<li>
<p><strong>XGBoost</strong> → When you need <strong>high accuracy</strong> and have time for <strong>hyperparameter tuning</strong>.</p>
</li>
<li>
<p><strong>LightGBM</strong> → When you have <strong>large datasets</strong> and need <strong>fast training</strong>.</p>
</li>
</ul>
<h3 id="i-model-uncertainty-bayesianprobabilistic-models"><strong>I. Model Uncertainty (Bayesian/Probabilistic Models)</strong><a class="headerlink" href="#i-model-uncertainty-bayesianprobabilistic-models" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Bayesian Neural Networks</strong>: Treats weights as probability distributions.</p>
</li>
<li>
<p><strong>Gaussian Processes</strong>: Models uncertainty in regression.</p>
</li>
<li>
<p><strong>Monte Carlo Dropout</strong>: Uses dropout at inference to approximate Bayesian uncertainty.</p>
</li>
</ul>
<h4 id="bayesian-regression"><strong>Bayesian Regression</strong><a class="headerlink" href="#bayesian-regression" title="Permanent link">&para;</a></h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.03729788959026337</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219161240.png" /></p>
<p><strong>Bayesian Regression</strong> is a probabilistic approach to linear regression. Instead of estimating a single set of parameters (as in ordinary least squares), it treats the parameters of the regression model as random variables with a prior distribution.</p>
<ol>
<li>
<p><strong>Model Assumptions</strong>:</p>
<ul>
<li>
<p>Assume a linear model for the data:</p>
<div class="arithmatex">\[
y = X\beta + \epsilon
\]</div>
<p>where:</p>
</li>
<li>
<p><span class="arithmatex">\(y\)</span> is the vector of observed values,</p>
</li>
<li>
<p><span class="arithmatex">\(X\)</span> is the design matrix (input features),</p>
</li>
<li>
<p><span class="arithmatex">\(\beta\)</span> is the vector of regression coefficients (parameters),</p>
</li>
<li>
<p><span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span> is the Gaussian noise with variance <span class="arithmatex">\(\sigma^2\)</span>.</p>
</li>
</ul>
</li>
<li>
<p><strong>Prior</strong>:</p>
<ul>
<li>Assume a <strong>Gaussian prior</strong> on the coefficients <span class="arithmatex">\(\beta\)</span>:</li>
</ul>
<div class="arithmatex">\[
p(\beta) = \mathcal{N}(\beta \mid 0, \tau^2 I)
\]</div>
<p>where <span class="arithmatex">\(\tau^2\)</span> is the prior variance of the coefficients.</p>
</li>
<li>
<p><strong>Likelihood</strong>:</p>
<ul>
<li>The likelihood function given the data is:</li>
</ul>
<div class="arithmatex">\[
p(y \mid X, \beta, \sigma^2) = \mathcal{N}(y \mid X\beta, \sigma^2 I)
\]</div>
<p>This is a Gaussian distribution with mean <span class="arithmatex">\(X\beta\)</span> and covariance <span class="arithmatex">\(\sigma^2 I\)</span>.</p>
</li>
<li>
<p><strong>Posterior</strong> (Bayes' Theorem):</p>
<ul>
<li>Using Bayes' Theorem, the posterior distribution of <span class="arithmatex">\(\beta\)</span> given the data is:</li>
</ul>
<div class="arithmatex">\[
p(\beta \mid X, y, \sigma^2) \propto p(y \mid X, \beta, \sigma^2) p(\beta)
\]</div>
</li>
<li>
<p><strong>Maximum Likelihood Estimation</strong>:</p>
<ul>
<li>
<p>To find the <strong>maximum likelihood estimates</strong> (MLE) of the parameters <span class="arithmatex">\(\beta\)</span> and <span class="arithmatex">\(\sigma^2\)</span>, we can maximize the likelihood function:</p>
<div class="arithmatex">\[
\mathcal{L}(\beta, \sigma^2 \mid X, y) = p(y \mid X, \beta, \sigma^2)
\]</div>
</li>
<li>
<p>The log-likelihood is:</p>
<div class="arithmatex">\[
\log \mathcal{L}(\beta, \sigma^2 \mid X, y) = -\frac{N}{2} \log (2\pi \sigma^2) - \frac{1}{2\sigma^2} \| y - X\beta \|^2
\]</div>
</li>
<li>
<p>To maximize this, take the derivative with respect to <span class="arithmatex">\(\beta\)</span> and <span class="arithmatex">\(\sigma^2\)</span>, set them to zero, and solve for <span class="arithmatex">\(\beta\)</span> and <span class="arithmatex">\(\sigma^2\)</span>.</p>
</li>
</ul>
</li>
</ol>
<h5 id="mle-for-beta"><strong>MLE for <span class="arithmatex">\(\beta\)</span></strong><a class="headerlink" href="#mle-for-beta" title="Permanent link">&para;</a></h5>
<ul>
<li>The MLE of <span class="arithmatex">\(\beta\)</span> (ignoring <span class="arithmatex">\(\sigma^2\)</span>) is the ordinary least squares (OLS) solution:</li>
</ul>
<div class="arithmatex">\[
\hat{\beta} = (X^T X)^{-1} X^T y
\]</div>
<h5 id="mle-for-sigma2"><strong>MLE for <span class="arithmatex">\(\sigma^2\)</span></strong><a class="headerlink" href="#mle-for-sigma2" title="Permanent link">&para;</a></h5>
<ul>
<li>After finding <span class="arithmatex">\(\hat{\beta}\)</span>, the MLE of <span class="arithmatex">\(\sigma^2\)</span> is:</li>
</ul>
<div class="arithmatex">\[
\hat{\sigma}^2 = \frac{1}{N} \| y - X\hat{\beta} \|^2
\]</div>
<h5 id="computational-techniques"><strong>Computational Techniques:</strong><a class="headerlink" href="#computational-techniques" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p><strong>Analytical Approach:</strong> In most cases (like when the prior and likelihood are Gaussian), the posterior distribution is also Gaussian, so you can compute it analytically. This is the standard approach for simple Bayesian linear regression.</p>
</li>
<li>
<p><strong>Markov Chain Monte Carlo (MCMC):</strong> For more complex models (e.g., non-linear regression or non-Gaussian priors), you can use MCMC methods (such as <strong>Metropolis-Hastings</strong> or <strong>Gibbs sampling</strong>) to sample from the posterior distribution of the parameters.</p>
</li>
<li>
<p><strong>Iterative Methods (Variational Inference):</strong> For more computationally complex models, you might use iterative methods like <strong>Variational Inference</strong> or <strong>Expectation-Maximization (EM)</strong> to approximate the posterior.</p>
</li>
</ul>
<hr />
<h4 id="gaussian-processes"><strong>Gaussian Processes</strong><a class="headerlink" href="#gaussian-processes" title="Permanent link">&para;</a></h4>
<p>A <strong>Gaussian Process</strong> (GP) is a <strong>non-parametric</strong> model used for regression tasks. Unlike typical regression models that assume a functional form (like linear regression), GPR assumes that the data is drawn from a <strong>Gaussian distribution</strong> over functions.</p>
<blockquote>
<p>[! Definition] Definition<br />
A <strong>Gaussian process</strong> is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by a <strong>mean function</strong> <span class="arithmatex">\(m(x)\)</span> and a <strong>covariance function</strong> (or kernel) <span class="arithmatex">\(k(x, x')\)</span>, which defines the correlation between inputs.</p>
</blockquote>
<div class="arithmatex">\[
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
\]</div>
<p>Where:</p>
<ul>
<li>
<p><span class="arithmatex">\(f(x)\)</span> is the function value at input <span class="arithmatex">\(x\)</span>,</p>
</li>
<li>
<p><span class="arithmatex">\(m(x)\)</span> is the <strong>mean function</strong> (often assumed to be zero),</p>
</li>
<li>
<p><span class="arithmatex">\(k(x, x')\)</span> is the <strong>covariance function</strong>, which defines the relationship between points <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(x'\)</span>.</p>
</li>
<li>
<p><strong>Model Assumptions</strong>:</p>
<ul>
<li>
<p>Assume the output <span class="arithmatex">\(y\)</span> is a realization of a <strong>Gaussian process</strong>:</p>
<div class="arithmatex">\[  
f(x) \sim \mathcal{GP}(0, k(x, x')) 
\]</div>
</li>
</ul>
<p>where <span class="arithmatex">\(k(x, x')\)</span> is a <strong>covariance kernel</strong> that defines the relationship between the points <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(x'\)</span>.</p>
</li>
<li>
<p><strong>Likelihood</strong>:</p>
<ul>
<li>
<p>Given <span class="arithmatex">\(N\)</span> training points <span class="arithmatex">\(X = \{x_1, x_2, …, x_N\}\)</span> and corresponding outputs <span class="arithmatex">\(y = \{y_1, y_2, …, y_N\}\)</span>, the likelihood of the observations is given by the <strong>multivariate normal distribution</strong>:</p>
<div class="arithmatex">\[  
p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
\]</div>
<p>where:</p>
</li>
<li>
<p><span class="arithmatex">\(K(X, X)\)</span> is the <strong>covariance matrix</strong> computed using the kernel function <span class="arithmatex">\(k(x, x')\)</span>,</p>
</li>
<li>
<p><span class="arithmatex">\(\sigma^2\)</span> is the noise variance, and</p>
</li>
<li>
<p><span class="arithmatex">\(I\)</span> is the identity matrix.</p>
</li>
</ul>
</li>
<li>
<p><strong>Maximizing the Likelihood</strong>:</p>
<ul>
<li>
<p>The log-likelihood is:</p>
<div class="arithmatex">\[  
\log p(y \mid X, \theta) = -\frac{1}{2} y^T (K(X, X) + \sigma^2 I)^{-1} y - \frac{1}{2} \log \det (K(X, X) + \sigma^2 I) - \frac{N}{2} \log 2\pi    
\]</div>
</li>
<li>
<p>Here, <span class="arithmatex">\(\theta\)</span> refers to the kernel hyperparameters (e.g., length scale, variance) and <span class="arithmatex">\(\sigma^2\)</span> is the noise parameter.</p>
</li>
</ul>
</li>
<li>
<p><strong>Optimization</strong>:</p>
<ul>
<li>
<p>To find the <strong>maximum likelihood estimate (MLE)</strong> of the kernel parameters <span class="arithmatex">\(\theta\)</span> and <span class="arithmatex">\(\sigma^2\)</span>, we maximize the log-likelihood:</p>
<div class="arithmatex">\[  
\hat{\theta}, \hat{\sigma^2} = \arg \max_\theta \log p(y \mid X, \theta)    
\]</div>
</li>
<li>
<p>This is typically done via numerical optimization techniques like gradient descent or conjugate gradient methods, since the log-likelihood is non-linear with respect to the kernel parameters.</p>
</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:</p>
<ul>
<li>
<p>Once the kernel parameters are optimized, predictions at a new test point <span class="arithmatex">\(x_*\)</span> can be made by conditioning the Gaussian process on the training data. The predictive mean and variance are given by:</p>
<div class="arithmatex">\[
\mu_* = k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} y
\]</div>
<div class="arithmatex">\[
\sigma_*^2 = k(x_*, x_*) - k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} k(X, x_*)
\]</div>
</li>
<li>
<p>Here, <span class="arithmatex">\(k(x_*, X)\)</span> is the covariance between the test point <span class="arithmatex">\(x_*\)</span> and the training points <span class="arithmatex">\(X\)</span>, and <span class="arithmatex">\(k(x_*, x_*)\)</span> is the covariance at the test point.</p>
</li>
</ul>
</li>
<li>
<p><strong>Model Definition:</strong></p>
<p>A Gaussian Process defines a <strong>prior</strong> over functions. We assume the function values <span class="arithmatex">\(f(x)\)</span> at any set of points <span class="arithmatex">\(X\)</span> follow a multivariate normal distribution:</p>
<div class="arithmatex">\[  
f(X) \sim \mathcal{N}(0, K(X, X))
\]</div>
<p>where <span class="arithmatex">\(K(X, X)\)</span> is the <strong>covariance matrix</strong> determined by the chosen kernel (covariance function). Common kernels include the <strong>RBF kernel</strong> (Radial Basis Function), <strong>Matern kernel</strong>, etc.</p>
</li>
<li>
<p><strong>Likelihood Function:</strong></p>
<p>Given the observations <span class="arithmatex">\(y = f(X) + \epsilon\)</span>, where <span class="arithmatex">\(\epsilon\)</span> is noise, the likelihood function is:</p>
<div class="arithmatex">\[  
p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
\]</div>
<p>where <span class="arithmatex">\(\sigma^2\)</span> is the noise variance.</p>
</li>
<li>
<p><strong>Posterior Distribution:</strong></p>
<p>The posterior distribution of the function values <span class="arithmatex">\(f_*\)</span> at new test points <span class="arithmatex">\(X_*\)</span>, given the training data <span class="arithmatex">\((X, y)\)</span>, is also Gaussian:</p>
<div class="arithmatex">\[  
p(f_* \mid X_*, X, y) = \mathcal{N}(f_* \mid \mu_*, \Sigma_*)   
\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(\mu_*\)</span> is the mean of the posterior, and</p>
</li>
<li>
<p><span class="arithmatex">\(\Sigma_*\)</span> is the covariance (uncertainty).</p>
</li>
</ul>
<p>The mean and covariance of the posterior are given by:</p>
<div class="arithmatex">\[  
\mu_* = K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} y  
\]</div>
<div class="arithmatex">\[  
\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} K(X, X_*) 
\]</div>
</li>
<li>
<p><strong>Iterative Methods:</strong></p>
<ul>
<li>In practice, <strong>iterative optimization</strong> (e.g., <strong>gradient descent</strong> or <strong>conjugate gradient methods</strong>) is used to find the optimal kernel hyperparameters by maximizing the log marginal likelihood.</li>
</ul>
</li>
</ul>
<h3 id="j-optimization-algorithms-improving-model-training"><strong>J. Optimization Algorithms (Improving Model Training)</strong><a class="headerlink" href="#j-optimization-algorithms-improving-model-training" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Gradient Descent Variants</strong>:</p>
<ul>
<li>
<p><strong>SGD (Stochastic Gradient Descent)</strong></p>
</li>
<li>
<p><strong>Momentum-based GD</strong></p>
</li>
<li>
<p><strong>Adam, RMSprop, Adagrad</strong></p>
</li>
</ul>
</li>
<li>
<p><strong>Second-order Optimization</strong>: Newton's method, Quasi-Newton methods (L-BFGS).</p>
</li>
<li>
<p><strong>Adaptive Learning Rate</strong>: Learning rate schedules (cosine annealing, warm restarts).</p>
</li>
</ul>
<h4 id="newtons-method"><strong>Newton's Method</strong><a class="headerlink" href="#newtons-method" title="Permanent link">&para;</a></h4>
<p>Newton's method uses the <strong>Hessian matrix</strong> (second derivative) for optimization:</p>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t - H^{-1} \nabla f(\theta)
\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(H\)</span> is the Hessian matrix (second derivative of the loss function).</p>
</li>
<li>
<p><span class="arithmatex">\(\nabla f(\theta)\)</span> is the gradient.</p>
</li>
</ul>
<h4 id="momentum-based-update"><strong>Momentum-based Update</strong><a class="headerlink" href="#momentum-based-update" title="Permanent link">&para;</a></h4>
<p>Momentum accelerates gradient descent by adding a fraction of the previous update to the current one:</p>
<div class="arithmatex">\[
v_t = \beta v_{t-1} - \alpha \nabla f(\theta_t)
\]</div>
<div class="arithmatex">\[
\theta_{t+1} = \theta_t + v_t
\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(v_t\)</span> is the velocity term,</p>
</li>
<li>
<p><span class="arithmatex">\(\beta\)</span> is the momentum coefficient (e.g., 0.9),</p>
</li>
<li>
<p><span class="arithmatex">\(\alpha\)</span> is the learning rate.
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">iterations</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># Momentum coefficient</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">order</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span>
    <span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>  <span class="c1"># Initialize velocity</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="n">params_grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>

        <span class="c1"># Update parameters using Momentum</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">):</span>
            <span class="n">velocity</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">velocity</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>  <span class="c1"># Apply momentum update</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">velocity</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>  <span class="c1"># Update weights</span>
</code></pre></div></td></tr></table></div></p>
</li>
</ul>
<h3 id="k-model-architecture-optimization-deep-learning"><strong>K. Model Architecture Optimization (Deep Learning)</strong><a class="headerlink" href="#k-model-architecture-optimization-deep-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Residual Networks (ResNet)</strong>: Helps with vanishing gradients.</p>
</li>
<li>
<p><strong>Transformers</strong>: Attention-based models (BERT, GPT).</p>
</li>
<li>
<p><strong>Neural Architecture Search (NAS)</strong>: Auto-tuning model structure.</p>
</li>
</ul>
<h3 id="l-interpretability-explainability"><strong>L. Interpretability &amp; Explainability</strong><a class="headerlink" href="#l-interpretability-explainability" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>SHAP, LIME</strong>: Explainable AI tools.</p>
</li>
<li>
<p><strong>Feature Importance Methods</strong>: Permutation importance, gradient-based saliency maps.</p>
</li>
</ul>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/your-username" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.annotate", "navigation.tabs", "navigation.indexes", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/math.js"></script>
      
    
  </body>
</html>