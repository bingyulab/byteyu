---
title: 01. Introduction
author: Bingyu Jiang
creation_date: 2025-02-18 18:14
modification_date: 星期二 18日 二月 2025 18:14:34
tags:
  - Note
---

# 01. Introduction

## Daily Quote 
> [!quote] When you are offended at any man's fault, turn to yourself and study your own failings. Then you will forget your anger.  
> — Epictetus

## Tag 
---

Machine learning (ML) is the **scientific study** of **algorithms** and <mark style="background: #FFB8EBA6;">statistical models</mark> that <mark style="background: #FF5582A6;">computer systems</mark> use to perform a specific task without using explicit instructions, relying on patterns and <mark style="background: #FFB86CA6;">inference</mark> instead. It is seen as a subset of <mark style="background: #BBFABBA6;">artificial intelligence</mark>. Machine learning algorithms build a <mark style="background: #ABF7F7A6;">mathematical model</mark> based on sample data, known as "<mark style="background: #ADCCFFA6;">training data</mark>", in order to make predictions or decisions without being explicitly programmed to perform the task.

The technique of using linear interpolation for tabulation was believed to be used by Babylonian astronomers and mathematicians in Seleucid Mesopotamia (last three centuries BC), and by the Greek astronomer and mathematician, Hipparchus (2nd century BC). A description of linear interpolation can be found in the Almagest (2nd century AD) by Ptolemy.  
![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250218182124.png)

---

## Regression

```python
import numpy.random as npr
import jax.numpy as np
from jax import grad
import matplotlib.pyplot as plt

# first generate some random data
X = npr.uniform(0, 1, 300)
true_w, true_b = 2, 1
# add some noise to the labels
Y = X*true_w + true_b + 0.2*npr.randn(300) + p.sin(2*np.pi*X)

plt.scatter(X, Y, marker='.', color='blue')
plt.plot(np.sort(X),  np.sort(X)*true_w + true_b + np.sin(2*np.pi*np.sort(X)), color='black')
plt.show()
```

![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219150927.png)

Regression is a fundamental concept in statistics and machine learning that models the relationship between a dependent variable (target) and one or more independent variables (features). It is widely used for **prediction, inference, and understanding data trends**.

---

### **1. Linear Regression**

Linear regression assumes a **linear relationship** between the input $X$ and output $Y$. The model is expressed as:

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$

where:

- $\beta_0$ is the **intercept** (constant term).

- $\beta_1$ is the **coefficient** (slope).

- $\epsilon$ is the **error term** (captures noise or randomness in data).

#### **Key Assumptions:**

1. **Linearity:** The relationship between $X$ and $Y$ is linear.

2. **Independence:** Observations are independent.

3. **Homoscedasticity:** The variance of errors remains constant across values of $X$.

4. **Normality of Errors:** The residuals (errors) follow a normal distribution.

#### **How It Works:**

Linear regression finds the **best-fit line** by minimizing the sum of squared errors, using **Ordinary Least Squares (OLS)**:

$$
\min_{\beta} \sum (Y_i - \hat{Y}_i)^2
$$

where $\hat{Y}_i$ is the predicted value.

---

### **2. Multiple Linear Regression**

Extends linear regression to **multiple features**:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon
$$

It captures the influence of multiple factors on the target variable.

---

### **3. Logistic Regression (For Classification)**

Despite the name, **logistic regression is used for classification**, not regression. It models the probability of a binary outcome:

$$
P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

- It uses the **logit (sigmoid) function** to map predictions between 0 and 1.

- **Generalized linear model (GLM)**: It assumes a **Bernoulli distribution** for $Y$.

---

### **4. Polynomial Regression**

If a relationship is **non-linear**, we extend linear regression to include polynomial terms:

$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_n X^n + \epsilon
$$

- It models curves instead of straight lines.

- The higher the polynomial degree, the better it fits, but **risk of overfitting** increases.

---

### **5. Generalized Linear Model (GLM)**

A **Generalized Linear Model (GLM)** extends ordinary linear regression by allowing for:

1. **Different Distributions**: The response variable $Y$ can follow different distributions (not just normal).

2. **Link Function**: Instead of modeling $Y$ directly, GLMs use a function to relate the mean $E[Y]$ to the linear predictor.

#### **General Form of GLM**

For a response variable $Y$, GLMs assume:

$$
g(E[Y]) = X\beta
$$

Where:

- $X$ is the matrix of input features.

- $\beta$ are the coefficients to learn.

- $g(\cdot)$ is the **link function**, which connects the expected value of $Y$ to a linear combination of inputs.

#### **Key Components of GLM**

1. **Exponential Family of Distributions**: $Y$ comes from a family of distributions (Normal, Poisson, Binomial, etc.).

2. **Link Function** $g(\cdot)$: Transforms $E[Y]$ into a linear model.

3. **Linear Predictor**: $X\beta$ remains a linear combination of inputs.

#### **Why GLMs?**

- Generalizes **linear regression** to different distributions.

- Uses **Maximum Likelihood Estimation (MLE)** instead of least squares.

- More flexible than simple linear regression.

---
### **Key Metrics for Evaluating Regression Models**

To measure the performance of a regression model, we use:

1. **Mean Squared Error (MSE):** Measures average squared differences between actual and predicted values.
	
	$$	
	MSE = \frac{1}{n} \sum (Y_i - \hat{Y}_i)^2	
	$$

2. **Root Mean Squared Error (RMSE):** Square root of MSE, which brings errors to the same unit as the data.

	$$
	RMSE = \sqrt{MSE}
	$$

3. **Mean Absolute Error (MAE):** Measures the average absolute differences.

	$$
	MAE = \frac{1}{n} \sum |Y_i - \hat{Y}_i|
	$$

4. **$R^2$ (Coefficient of Determination):** Explains the proportion of variance explained by the model.
	
	$$
	R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
	$$
---


```python
# the linear model
def linear(params, x):
    w,b = params
    return w*x + b

def loss(params, dataset):
    x, y = dataset
    pred = linear(params, x)
    return np.square(pred - y).mean()

# gradient function
loss_grad = grad(loss)

iterations = 500
step_size = 0.1
dataset = (X, Y)
w, b = 1.5, 2. # initial values for the parameters
for i in range(iterations):
    params = (w, b)
    loss_ = loss(params, dataset)
    # compute gradient w.r.t model parameters
    params_grad = loss_grad(params, dataset)
    # update parameters
    w -= step_size * params_grad[0]
    b -= step_size * params_grad[1]
    #print(loss_)

print("loss: {}, params {}".format(loss_, params))
    
plt.scatter(X, Y, marker='.', color='blue')
plt.plot(np.sort(X),  np.sort(X)*true_w + true_b + np.sin(2*np.pi*np.sort(X)), color='black')
plt.plot(np.sort(X), linear(params, np.sort(X)), color='red')
plt.show()
```
> loss: 0.25392094254493713,  
> params (Array(0.26151434, dtype=float32, weak_type=True), Array(1.8786527, dtype=float32, weak_type=True))


![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219151017.png)


## Optimization 

To **reduce RMSE**, we can use different strategies:

### **A. Dataset Improvements**

- **More Data**: Collect additional samples to improve generalization.

- **Data Augmentation**: Create synthetic data using transformations (e.g., image rotations, SMOTE for imbalanced data).

- **Synthetic Data Generation**: GANs, Variational Autoencoders, or bootstrapping.

- **Handling Missing Data**: Imputation methods (mean, median, KNN imputation).

- **Class Balancing**: Oversampling, undersampling, or synthetic data generation (e.g., SMOTE for imbalanced datasets).
### **B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)**

- **Choose the Right Model**: Simple models reduce overfitting; complex models improve accuracy but need regularization.

- **Polynomial Features**: Increase model complexity for non-linearity.

- **Early Stopping**: Stop training when validation loss starts increasing.

#### **VC Dimension (Vapnik-Chervonenkis Dimension)**

VC dimension is a fundamental concept in statistical learning theory that measures the **capacity** or **complexity** of a model class. It quantifies how well a model can **shatter** (perfectly classify) different sets of data points.

> [!Definition]  
The **VC dimension** of a hypothesis class $\mathcal{H}$ is the largest number of points that can be **shattered** by some hypothesis in $\mathcal{H}$.

- A model **shatters** a dataset if it can classify all possible labelings (all $2^n$ assignments of class labels for $n$ points).

- Higher VC dimension → More complex model → Higher capacity to fit data (risk of overfitting).

- Example:
	- A linear classifier in 2D can **shatter** at most **3** points, so its VC dimension is 3.
	
	- A linear classifier in $d$-dimensions has VC dimension $d+1$.

**Implications:**

- **Lower VC dimension** → Model is simple (underfitting risk).

- **Higher VC dimension** → Model is complex (overfitting risk).

- **VC Theorem**: If a model has a finite VC dimension $d$, it generalizes well if the number of training examples $n$ is sufficiently larger than $d$.

---

#### **Other Ways to Measure Model Complexity**

1. **Rademacher Complexity**

	- Measures how well a hypothesis class can fit random noise.
	
	- If a model class has high Rademacher complexity, it can overfit random labels.

2. **PAC-Bayes**, 

3. **margin-based bounds**	

4. **Geometric way**

---

- If the model is **too simple**, it underfits. We can:

- Use polynomial regression instead of linear.

- Add interaction terms.

- Use **nonlinear models** (e.g., random forests, neural networks).
```Python
# the nonlinear model
def nonlinear(params, x):
    res = 0.0
    for idx, weight in enumerate(params):
        res += weight * x ** idx
    return res

def loss(params, dataset):
    x, y = dataset
    pred = nonlinear(params, x)
    return np.square(pred - y).mean()

# gradient function
loss_grad = grad(loss)

dataset = (X, Y)
for order in range(2, 15, 2):
    weights = np.ones(order) * 1.5 
    
    for i in range(iterations):
        params = weights
        loss_ = loss(params, dataset)
        params_grad = loss_grad(params, dataset)
        # update parameters
        for j in range(order):
            weights = weights.at[j].set(weights[j] - step_size * params_grad[j])
    
    print("Order: {}, loss: {}".format(order, loss_))
```

> Order: 2, loss: 0.25392  
> Order: 4, loss: 0.24112  
> Order: 6, loss: 0.19777  
> Order: 8, loss: 0.16633  
> Order: 10, loss: 0.15277  
> Order: 12, loss: 0.15003  
> Order: 14, loss: 0.15189

![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154334.png)


### **C. Regularization (Avoid Overfitting)**

- **L1 Regularization (Lasso $\ell_1$)**: Encourages sparsity (feature selection).

- **L2 Regularization ($\ell_2$ Ridge)**: Reduces large weights, preventing overfitting.

- **Elastic Net**: Combination of L1 and L2.

- **Dropout (Neural Networks)**: Randomly removes neurons during training.

- **Batch Normalization**: Normalizes activations in deep networks.

---

#### Ridge regression(L2 regulation)

```Python
from sklearn import linear_model

idx = np.argsort(X)

# encoder
Z = np.concatenate([X[idx].reshape(-1, 1)**n for n in range(1, 10)], axis=1)

# ridge regression
reg = linear_model.Ridge(alpha=1e-8)
reg.fit(Z, Y[idx])
```

> Mean Squared Error (Loss): 0.04097932204604149

![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154350.png)

**Ridge regression** is a type of **linear regression** that includes an $L_2$-norm penalty to prevent overfitting. The objective function is:

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

where:

- $y_i$ are the observed values,

- $X_i$ are the input features,

- $\beta$ are the regression coefficients,

- $\lambda$ is the **regularization parameter**, which controls the trade-off between fitting the data and keeping coefficients small.

#### **Intuition:**

- Adding $\lambda \sum \beta_j^2$ **shrinks** the coefficients $\beta_j$ towards zero but **never exactly zero**.

- Ridge regression helps when features are **highly correlated**, reducing variance and preventing overfitting.

#### **Relation to Prior Distribution (Bayesian Interpretation)**

Ridge regression can be seen as **Bayesian linear regression** with a **Gaussian prior** on the coefficients:

$$

\beta_j \sim \mathcal{N}(0, \tau^2)

$$

where $\tau^2 = \frac{1}{\lambda}$. This means:

- The model assumes that **larger coefficients are less likely**, which enforces shrinkage.

- A **stronger prior (larger $\lambda$)** shrinks the coefficients more.

---

#### Lasso Regression(L1 Regulation)


```python
reg = linear_model.Lasso(alpha=1e-8)
reg.fit(Z, Y[idx])

predictions = reg.predict(Z)
mse = np.mean((predictions - Y[idx]) ** 2)
print(f"Mean Squared Error (Loss): {mse}")
```
> Mean Squared Error (Loss): 0.043433766812086105


**Lasso (Least Absolute Shrinkage and Selection Operator)** is another **regularized regression** method, but it uses an $L_1$-norm penalty instead of $L_2$:

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
$$

#### **Intuition:**

- The **absolute value** penalty forces some coefficients to be exactly **zero**, effectively performing **feature selection**.

- Unlike Ridge regression, Lasso can create **sparse models**, making it useful when there are many irrelevant features.

#### **Relation to Prior Distribution**

Lasso regression corresponds to **Bayesian regression** with a **Laplace prior**:

$$
\beta_j \sim \text{Laplace}(0, b)
$$

where $b = \frac{1}{\lambda}$. The **Laplace distribution** is sharply peaked at zero, which encourages sparsity. This is why Lasso forces some coefficients to be exactly **zero**.

#### **Why Does L1 Regularization Perform Feature Selection?**

1. **Sparsity Effect:**
	
	- The L1 penalty encourages some coefficients $\beta_j$ to be exactly **zero**.
	
	- This leads to a **sparse** model, where only a subset of the features are retained.
	
	- Features with zero coefficients are effectively removed from the model.

2. **Optimization Property:**
	
	- The L1 norm creates a **non-differentiable** point at $\beta_j = 0$, which forces some coefficients to shrink to zero.
	
	- Geometrically, the constraint region forms a **diamond shape**, which makes it more likely for the optimal solution to lie on the axes (i.e., some coefficients are zero).

3. **Automatic Feature Selection:**

	- Unlike L2 regularization (**Ridge regression**), which only shrinks coefficients towards zero, L1 **eliminates** irrelevant features entirely.
	
	- This is useful when you have **high-dimensional data** with many irrelevant or redundant features.

---


### Elastic Net Regression

**Elastic Net** combines both Ridge ($L_2$) and Lasso ($L_1$) penalties:

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
$$

where:

- $\lambda_1$ controls the Lasso part (feature selection).

- $\lambda_2$ controls the Ridge part (shrinkage and handling collinearity).

### **Intuition:**

- If features are **highly correlated**, Lasso can randomly pick one and discard the others. Elastic Net avoids this issue by keeping a mix of features.

- It **selects features like Lasso** while **shrinking coefficients like Ridge**.

#### **Relation to Prior Distribution**

Elastic Net corresponds to a **Mixture Prior**:

- It assumes a **combination** of **Gaussian (Ridge) and Laplace (Lasso) priors** on the coefficients.

- This means that some coefficients get **shrunk** (like Ridge), while others get **sparsified** (like Lasso).

---

### **D. Feature Engineering & Selection**

- **One-hot Encoding**: Convert categorical variables into numerical.

- **Standardization (Z-score, Min-Max Scaling)**: Normalize feature values.
- Other **feature transformations** (log, power, etc.).

- **Feature Selection Methods**:

	- **Univariate Tests**: Select top-ranked features based on correlation.
	
	- **Recursive Feature Elimination (RFE)**: Iteratively remove less important features.
	- **Lasso (L1 Regularization)**: Selects only important features.
	- Remove **irrelevant or redundant features** to reduce noise.


```python
dataset = (X, np.log(Y))

# omit repeated code
plt.plot(np.sort(X), np.exp(linear(params, np.sort(X))), color='red')
```
> loss: 0.07207518070936203

![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154941.png)

- Perform **dimensionality reduction** (PCA, autoencoders).

### **E. Dimensionality Reduction**

- **PCA (Principal Component Analysis)**: Reduces correlated features.

- **LDA (Linear Discriminant Analysis)**: Used in classification tasks.

- **t-SNE & UMAP**: Nonlinear dimensionality reduction for visualization.

### **F. Reduce Outliers & Noise**

- **Z-score, IQR Method**: Remove extreme outliers.

- **Robust Loss Functions**: Huber loss, quantile loss.

- **Denoising Methods**: Gaussian smoothing, moving average.
	- RMSE is sensitive to **outliers**. 

```Python
from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.05)  
outliers = iso_forest.fit_predict(X.reshape(-1, 1))

inliers = outliers == 1

X_filtered = X[inliers]
Y_filtered = Y[inliers]
```
> Mean Squared Error (Loss): 0.11134050786495209

- Robust regression (Huber loss).
```python
reg = linear_model.HuberRegressor(alpha=0.0, epsilon=1.5)
```
> Mean Squared Error (Loss): 0.03720296174287796

The **Huber loss** is a combination of **squared loss** (used in ordinary least squares) and **absolute loss** (used in robust regression), designed to handle **outliers** more gracefully. It is particularly useful when you want a model that is robust to **outliers** but still penalizes small errors quadratically, like the regular least squares.

> [!Definition] Definition  
> The Huber loss function $L_\delta(y, \hat{y})$ is defined as:
> $$
L_\delta(y, \hat{y}) =  
\begin{cases}  
\frac{1}{2}(y - \hat{y})^2, & \text{for} \ |y - \hat{y}| \leq \delta \\  
\delta |y - \hat{y}| - \frac{1}{2} \delta^2, & \text{for} \ |y - \hat{y}| > \delta  
\end{cases}
> $$
> where:
> - $y$ is the true value,
> - $\hat{y}$ is the predicted value,
> - $\delta$ is a threshold that controls the transition between quadratic and linear loss.

#### **Intuition:**

- For errors less than or equal to $\delta$, the loss behaves like a **squared error** (quadratic), which heavily penalizes small deviations.

- For large errors (i.e., when the error is greater than $\delta$), the loss behaves like **absolute error**, which grows linearly, avoiding excessive penalties for large deviations or outliers.

This makes the **Huber loss** more **robust to outliers** compared to squared error (which can be highly influenced by large deviations).

### **G. Optimize Hyperparameters**

- **Grid Search**: Exhaustively tries all combinations.

- **Random Search**: Randomly samples hyperparameters.

- **Bayesian Optimization**: Uses probability to find optimal hyperparameters.

- **Evolutionary Algorithms**: Genetic algorithms to find the best configuration.

#### **Grid Search**

- Exhaustively searches over a predefined set of hyperparameters.

- Creates a **grid** of possible values and evaluates each combination.

**Example:**

If we have:

- Learning rate $\eta$ in {0.001, 0.01, 0.1}

- Regularization $\lambda$ in {0.001, 0.01, 0.1}

Grid search tests all $3 \times 3 = 9$ combinations.

✅ **Pros:**

- Guarantees best result within search space.

- Simple and systematic.

❌ **Cons:**

- Computationally **expensive**.

- Wasteful if only a few hyperparameters matter.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [10, 50, 100],
    'max_depth': [3, 5, 10],
    'min_samples_split': [2, 5, 10]
}

clf = RandomForestClassifier()
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)

```

---

#### **Random Search**

- Randomly samples hyperparameter values from a distribution.

- Does not evaluate all combinations but instead explores the space **stochastically**.

**Example:**

- Instead of testing all 9 combinations, randomly select 5 combinations.

✅ **Pros:**

- Often more **efficient** than grid search.

- Works well when **only a few hyperparameters significantly affect performance**.

- **More effective in high-dimensional spaces**.

❌ **Cons:**
- No guarantee of finding the absolute best hyperparameters.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(10, 200),
    'max_depth': randint(3, 20),
    'min_samples_split': randint(2, 20)
}

clf = RandomForestClassifier()
random_search = RandomizedSearchCV(clf, param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X_train, y_train)

print("Best parameters:", random_search.best_params_)
print("Best score:", random_search.best_score_)

```

---

#### **Bayesian Optimization (BO)**

Grid and random search **do not use past evaluations** to guide future searches. **Bayesian Optimization (BO)** does.

##### **How It Works**

1. Define a **prior** belief about the function mapping hyperparameters to performance.

2. Use a **surrogate model** (usually a **Gaussian Process**) to estimate the objective function.

3. Select the next hyperparameter set based on an **acquisition function** (e.g., Expected Improvement, Upper Confidence Bound).

4. Evaluate the model with these hyperparameters.

5. Update the belief and repeat.

---

#### **Key Components of BO:**

- **Surrogate Model:**	
	- Typically a **Gaussian Process (GP)** that models the unknown function.
	
	- Uses previous evaluations to estimate performance at new points.

- **Acquisition Function:**	
	- Guides the search by balancing **exploration** (trying new areas) and **exploitation** (focusing on promising regions).

- Common choices:	
	- **Expected Improvement (EI)**: Pick points expected to improve the best result.
	
	- **Upper Confidence Bound (UCB)**: Pick points with high uncertainty.

```python
from bayes_opt import BayesianOptimization

def rf_cv(n_estimators, max_depth, min_samples_split):
    model = RandomForestClassifier(
        n_estimators=int(n_estimators),
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        random_state=42
    )
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)  # Optimization objective

# Define hyperparameter space
pbounds = {
    'n_estimators': (10, 200),
    'max_depth': (3, 20),
    'min_samples_split': (2, 20)
}

optimizer = BayesianOptimization(
    f=rf_cv,
    pbounds=pbounds,
    random_state=42
)

optimizer.maximize(init_points=5, n_iter=20)

print("Best parameters found:", optimizer.max)
```

---

##### **Why Bayesian Optimization is Powerful?**

✅ **More efficient than grid/random search** → Finds good hyperparameters in fewer evaluations.

✅ **Works well for expensive models** (e.g., deep learning, reinforcement learning).

✅ **Uses prior knowledge** to guide search intelligently.

❌ **Downside?**

- Computationally **expensive** for very high-dimensional problems.

- Assumes a smooth objective function, which may not always hold.

---

#### **Comparison of Methods**

| Method | Strengths | Weaknesses |
|------------------------|----------------------------------------------------|------------------------------------------------|
| **Grid Search** | Systematic, finds best result within search space | Expensive, infeasible in high dimensions |
| **Random Search** | More efficient than grid search, better exploration | No guarantee of best hyperparameters |
| **Bayesian Optimization** | Uses prior knowledge, efficient in low-data settings | Computationally expensive in high dimensions |

---

### **(6) Ensemble Methods**

- **Bagging** (Bootstrap Aggregating): Train multiple models on bootstrapped datasets (e.g., Random Forest).

- **Boosting**: Sequential training that corrects previous errors (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM).

- **Stacking**: Combine multiple models via a meta-model.

#### Random Forest
```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
```
> Mean Squared Error (Loss): 0.008086484856903553

![](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219172219.png)

Random Forest is an **ensemble learning method** that builds multiple decision trees and combines their outputs to improve prediction accuracy and reduce overfitting.

**How Random Forest Works**

- **Bootstrap Aggregation (Bagging):**

- Random Forest trains multiple decision trees on different **bootstrapped samples** of the dataset.

- Each tree is trained on a **random subset** (without replacement) of the features.

- The predictions from all trees are **averaged** (for regression) or use **majority voting** (for classification).

**Why it Works:**

- Reduces **overfitting** (compared to a single decision tree).

- Handles **high-dimensional data** well.

- **Robust to noise** and missing values.

**Pros & Cons**

✅ Reduces variance (compared to individual trees).

✅ Handles categorical & numerical features well.

✅ Works well for large datasets.

❌ Slower than a single decision tree.

❌ Large ensembles can be computationally expensive.


#### GBDT (Gradient Boosted Decision Trees)
```python
import lightgbm as lgb

lgb_train = lgb.Dataset(Z, label=Y[idx].tolist())
params = {'objective': 'regression', 'metric': 'mse', 'boosting_type': 'gbdt', 'verbose': -1}
lgb_model = lgb.train(params, lgb_train, num_boost_round=100)
predictions = lgb_model.predict(Z)
```
> Mean Squared Error (Loss): 0.19826513528823853

![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219172346.png)

GBDT (Gradient Boosted Decision Trees) is a **powerful ensemble learning method** that builds multiple decision trees sequentially to **minimize a given loss function** using gradient descent. It is widely used in machine learning for regression, classification, and ranking problems.

**How GBDT Works**

GBDT is based on the **boosting** technique, where models are trained **sequentially**, and each tree tries to correct the errors made by the previous ones.

##### **Step-by-Step Process:**

1. **Start with a weak model**

	- Usually, this is a simple decision tree (often a shallow one, also called a weak learner).
	
	- The first tree predicts the target values roughly.

2. **Compute the residual errors (gradients of the loss function)**
	
	- For regression:
	
	$$
	r_i = y_i - \hat{y}_i
	$$
	
	- For classification:
	
	$$
	r_i = -\frac{\partial L(y, \hat{y})}{\partial \hat{y}}
	$$
	
	- The model learns the difference (residuals) between the predicted and actual values.

3. **Train a new decision tree to predict the residuals**
	
	- This new tree focuses on correcting the errors made by the previous one.

4. **Update the prediction**
	
	- Add the new tree's predictions to the overall model:
	
	$$
	F_{m+1}(x) = F_m(x) + \eta h_m(x)
	$$
	
	- $\eta$ is the **learning rate**, which controls how much each tree contributes to the final prediction.

5. **Repeat the process**
	
	- Keep adding new trees until a stopping criterion is met (e.g., a fixed number of trees or no significant improvement in performance).


##### **Why GBDT Works Well**

- **Handles Non-Linearity:** Unlike linear models, GBDT captures complex, non-linear relationships.

- **Adaptive Learning:** Each new tree corrects errors made by previous ones.

- **Robust to Outliers:** GBDT can be robust if loss functions like Huber loss are used.

- **Feature Importance:** It naturally provides feature importance rankings.

---

#### **XGBoost (Extreme Gradient Boosting)**

XGBoost is an optimized implementation of **gradient boosting** that is highly efficient and widely used in machine learning competitions.

**How XGBoost Works**

- **Gradient Boosting:**
	
	- Unlike Random Forest (which trains trees independently), XGBoost **trains trees sequentially**, where each new tree corrects the errors of the previous ones.
	
	- It **minimizes a loss function** using **gradient descent**, hence the name "gradient boosting."

- **Key Optimizations:**

	- **Regularization:** Adds L1/L2 penalties (like Lasso/Ridge) to prevent overfitting.
	
	- **Tree Pruning:** Uses a **maximum depth** instead of splitting until pure.
	
	- **Handling Missing Values:** Automatically learns best imputation.
	
	- **Parallelization:** Efficiently builds trees using multi-threading.

**Pros & Cons**

✅ Faster than traditional gradient boosting (due to optimizations).

✅ Handles large-scale data efficiently.

✅ Highly customizable with tunable hyperparameters.

❌ Can be prone to overfitting if not tuned properly.

❌ Sensitive to hyperparameter settings.

---

#### **LightGBM (Light Gradient Boosting Machine)**

LightGBM is another gradient boosting library, optimized for **speed** and **efficiency**.

**How LightGBM Works**

- **Key Differences from XGBoost:**

- **Leaf-wise tree growth:**

	- XGBoost grows trees level-wise (uniform expansion).
	
	- LightGBM grows trees leaf-wise (expands the most promising leaf first), leading to deeper trees and faster convergence.

- **Histogram-based splitting:**

	- Instead of checking every data point for splits, LightGBM groups values into **histogram bins**, reducing computation.

- **Better memory efficiency:**

	- Uses **less RAM** than XGBoost due to histogram-based processing.

**Pros & Cons**

✅ Much **faster** than XGBoost, especially for large datasets.

✅ **Handles categorical features** without needing one-hot encoding.

✅ Scales well to large datasets.

❌ May be **less interpretable** than XGBoost.

❌ Can overfit if not properly tuned.

**When to Use Which?**

- **Random Forest** → When you need an **interpretable model** and have **limited data**.

- **XGBoost** → When you need **high accuracy** and have time for **hyperparameter tuning**.

- **LightGBM** → When you have **large datasets** and need **fast training**.
### **I. Model Uncertainty (Bayesian/Probabilistic Models)**
- **Bayesian Neural Networks**: Treats weights as probability distributions.

- **Gaussian Processes**: Models uncertainty in regression.

- **Monte Carlo Dropout**: Uses dropout at inference to approximate Bayesian uncertainty.

#### **Bayesian Regression**
```python
reg = linear_model.BayesianRidge()
```
> Mean Squared Error (Loss): 0.03729788959026337

![image.png](https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219161240.png)

**Bayesian Regression** is a probabilistic approach to linear regression. Instead of estimating a single set of parameters (as in ordinary least squares), it treats the parameters of the regression model as random variables with a prior distribution.

1. **Model Assumptions**:

	- Assume a linear model for the data:
	
		$$
		y = X\beta + \epsilon
		$$
		
		where:
	
	- $y$ is the vector of observed values,
	
	- $X$ is the design matrix (input features),
	
	- $\beta$ is the vector of regression coefficients (parameters),
	
	- $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ is the Gaussian noise with variance $\sigma^2$.

2. **Prior**:
	
	- Assume a **Gaussian prior** on the coefficients $\beta$:
	
	$$
	p(\beta) = \mathcal{N}(\beta \mid 0, \tau^2 I)
	$$
	
	where $\tau^2$ is the prior variance of the coefficients.

3. **Likelihood**:
	
	- The likelihood function given the data is:
	
	$$
	p(y \mid X, \beta, \sigma^2) = \mathcal{N}(y \mid X\beta, \sigma^2 I)
	$$
	
	This is a Gaussian distribution with mean $X\beta$ and covariance $\sigma^2 I$.

4. **Posterior** (Bayes' Theorem):
	
	- Using Bayes' Theorem, the posterior distribution of $\beta$ given the data is:
	
	$$
	p(\beta \mid X, y, \sigma^2) \propto p(y \mid X, \beta, \sigma^2) p(\beta)
	$$

5. **Maximum Likelihood Estimation**:
	
	- To find the **maximum likelihood estimates** (MLE) of the parameters $\beta$ and $\sigma^2$, we can maximize the likelihood function:
	
		$$
		\mathcal{L}(\beta, \sigma^2 \mid X, y) = p(y \mid X, \beta, \sigma^2)
		$$
	
	- The log-likelihood is:
	
		$$
		\log \mathcal{L}(\beta, \sigma^2 \mid X, y) = -\frac{N}{2} \log (2\pi \sigma^2) - \frac{1}{2\sigma^2} \| y - X\beta \|^2
		$$
	
	- To maximize this, take the derivative with respect to $\beta$ and $\sigma^2$, set them to zero, and solve for $\beta$ and $\sigma^2$.

##### **MLE for $\beta$**

- The MLE of $\beta$ (ignoring $\sigma^2$) is the ordinary least squares (OLS) solution:

$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$

##### **MLE for $\sigma^2$**

- After finding $\hat{\beta}$, the MLE of $\sigma^2$ is:

$$
\hat{\sigma}^2 = \frac{1}{N} \| y - X\hat{\beta} \|^2
$$

##### **Computational Techniques:**

- **Analytical Approach:** In most cases (like when the prior and likelihood are Gaussian), the posterior distribution is also Gaussian, so you can compute it analytically. This is the standard approach for simple Bayesian linear regression.

- **Markov Chain Monte Carlo (MCMC):** For more complex models (e.g., non-linear regression or non-Gaussian priors), you can use MCMC methods (such as **Metropolis-Hastings** or **Gibbs sampling**) to sample from the posterior distribution of the parameters.

- **Iterative Methods (Variational Inference):** For more computationally complex models, you might use iterative methods like **Variational Inference** or **Expectation-Maximization (EM)** to approximate the posterior.

---

#### **Gaussian Processes**

A **Gaussian Process** (GP) is a **non-parametric** model used for regression tasks. Unlike typical regression models that assume a functional form (like linear regression), GPR assumes that the data is drawn from a **Gaussian distribution** over functions.

> [! Definition] Definition  
A **Gaussian process** is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by a **mean function** $m(x)$ and a **covariance function** (or kernel) $k(x, x')$, which defines the correlation between inputs.

$$
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
$$

Where:

- $f(x)$ is the function value at input $x$,

- $m(x)$ is the **mean function** (often assumed to be zero),

- $k(x, x')$ is the **covariance function**, which defines the relationship between points $x$ and $x'$.

1. **Model Assumptions**:
	
	- Assume the output $y$ is a realization of a **Gaussian process**:
	
		$$	
		f(x) \sim \mathcal{GP}(0, k(x, x'))	
		$$
	
	where $k(x, x')$ is a **covariance kernel** that defines the relationship between the points $x$ and $x'$.

2. **Likelihood**:
	
	- Given $N$ training points $X = \{x_1, x_2, …, x_N\}$ and corresponding outputs $y = \{y_1, y_2, …, y_N\}$, the likelihood of the observations is given by the **multivariate normal distribution**:
	
		$$	
		p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
		$$
	
		where:
	
	- $K(X, X)$ is the **covariance matrix** computed using the kernel function $k(x, x')$,
	
	- $\sigma^2$ is the noise variance, and
	
	- $I$ is the identity matrix.

3. **Maximizing the Likelihood**:
	
	- The log-likelihood is:
	
		$$	
		\log p(y \mid X, \theta) = -\frac{1}{2} y^T (K(X, X) + \sigma^2 I)^{-1} y - \frac{1}{2} \log \det (K(X, X) + \sigma^2 I) - \frac{N}{2} \log 2\pi	
		$$
	
	- Here, $\theta$ refers to the kernel hyperparameters (e.g., length scale, variance) and $\sigma^2$ is the noise parameter.

4. **Optimization**:
	
	- To find the **maximum likelihood estimate (MLE)** of the kernel parameters $\theta$ and $\sigma^2$, we maximize the log-likelihood:
	
		$$	
		\hat{\theta}, \hat{\sigma^2} = \arg \max_\theta \log p(y \mid X, \theta)	
		$$
		
	- This is typically done via numerical optimization techniques like gradient descent or conjugate gradient methods, since the log-likelihood is non-linear with respect to the kernel parameters.

5. **Prediction**:
	
	- Once the kernel parameters are optimized, predictions at a new test point $x_*$ can be made by conditioning the Gaussian process on the training data. The predictive mean and variance are given by:
	
		$$
		\mu_* = k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} y
		$$
		
		$$
		\sigma_*^2 = k(x_*, x_*) - k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} k(X, x_*)
		$$
	
	- Here, $k(x_*, X)$ is the covariance between the test point $x_*$ and the training points $X$, and $k(x_*, x_*)$ is the covariance at the test point.

6. **Model Definition:**
	
	A Gaussian Process defines a **prior** over functions. We assume the function values $f(x)$ at any set of points $X$ follow a multivariate normal distribution:
	
	$$	
	f(X) \sim \mathcal{N}(0, K(X, X))
	$$
	
	where $K(X, X)$ is the **covariance matrix** determined by the chosen kernel (covariance function). Common kernels include the **RBF kernel** (Radial Basis Function), **Matern kernel**, etc.

7. **Likelihood Function:**
	
	Given the observations $y = f(X) + \epsilon$, where $\epsilon$ is noise, the likelihood function is:
	
	$$	
	p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
	$$
	
	where $\sigma^2$ is the noise variance.

8. **Posterior Distribution:**
	
	The posterior distribution of the function values $f_*$ at new test points $X_*$, given the training data $(X, y)$, is also Gaussian:
	
	$$	
	p(f_* \mid X_*, X, y) = \mathcal{N}(f_* \mid \mu_*, \Sigma_*)	
	$$
	
	where:
	
	- $\mu_*$ is the mean of the posterior, and
	
	- $\Sigma_*$ is the covariance (uncertainty).
	
	The mean and covariance of the posterior are given by:
	
	$$	
	\mu_* = K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} y	
	$$
	
	$$	
	\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} K(X, X_*)	
	$$

9. **Iterative Methods:**

	- In practice, **iterative optimization** (e.g., **gradient descent** or **conjugate gradient methods**) is used to find the optimal kernel hyperparameters by maximizing the log marginal likelihood.

### **J. Optimization Algorithms (Improving Model Training)**

- **Gradient Descent Variants**:
	- **SGD (Stochastic Gradient Descent)**
	
	- **Momentum-based GD**
	
	- **Adam, RMSprop, Adagrad**

- **Second-order Optimization**: Newton's method, Quasi-Newton methods (L-BFGS).

- **Adaptive Learning Rate**: Learning rate schedules (cosine annealing, warm restarts).

#### **Newton's Method**

Newton's method uses the **Hessian matrix** (second derivative) for optimization:

$$
\theta_{t+1} = \theta_t - H^{-1} \nabla f(\theta)
$$

where:

- $H$ is the Hessian matrix (second derivative of the loss function).

- $\nabla f(\theta)$ is the gradient.

#### **Momentum-based Update**

Momentum accelerates gradient descent by adding a fraction of the previous update to the current one:

$$
v_t = \beta v_{t-1} - \alpha \nabla f(\theta_t)
$$

$$
\theta_{t+1} = \theta_t + v_t
$$

where:

- $v_t$ is the velocity term,

- $\beta$ is the momentum coefficient (e.g., 0.9),

- $\alpha$ is the learning rate.
```Python
iterations = 500
step_size = 0.1
beta = 0.9  # Momentum coefficient
dataset = (X, Y)

for order in range(2, 15, 2):
    weights = np.ones(order) * 1.5
    velocity = np.zeros(order)  # Initialize velocity

    for i in range(iterations):
        params = weights
        loss_ = loss(params, dataset)
        params_grad = loss_grad(params, dataset)

        # Update parameters using Momentum
        for j in range(order):
            velocity[j] = beta * velocity[j] - step_size * params_grad[j]  # Apply momentum update
            weights = weights.at[j].set(weights[j] + velocity[j])  # Update weights

```
### **K. Model Architecture Optimization (Deep Learning)**

- **Residual Networks (ResNet)**: Helps with vanishing gradients.

- **Transformers**: Attention-based models (BERT, GPT).

- **Neural Architecture Search (NAS)**: Auto-tuning model structure.

### **L. Interpretability & Explainability**

- **SHAP, LIME**: Explainable AI tools.

- **Feature Importance Methods**: Permutation importance, gradient-based saliency maps.
