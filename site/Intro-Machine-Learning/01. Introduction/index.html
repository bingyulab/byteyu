
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Bingyu Jiang">
      
      
      
        <link rel="prev" href="../..">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.5">
    
    
      
        <title>01. Introduction - byteyu</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../stylesheets/code.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#01-introduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="byteyu" class="md-header__button md-logo" aria-label="byteyu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            byteyu
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              01. Introduction
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="byteyu" class="md-nav__button md-logo" aria-label="byteyu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    byteyu
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome to bteyu
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Intro Machine Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Intro Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    01. Introduction
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    01. Introduction
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#daily-quote" class="md-nav__link">
    <span class="md-ellipsis">
      Daily Quote
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tag" class="md-nav__link">
    <span class="md-ellipsis">
      Tag
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    <span class="md-ellipsis">
      Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      1. Linear Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Linear Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-assumptions" class="md-nav__link">
    <span class="md-ellipsis">
      Key Assumptions:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-multiple-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      2. Multiple Linear Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-logistic-regression-for-classification" class="md-nav__link">
    <span class="md-ellipsis">
      3. Logistic Regression (For Classification)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-polynomial-regression" class="md-nav__link">
    <span class="md-ellipsis">
      4. Polynomial Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-generalized-linear-model-glm" class="md-nav__link">
    <span class="md-ellipsis">
      5. Generalized Linear Model (GLM)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Generalized Linear Model (GLM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-form-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      General Form of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-glms" class="md-nav__link">
    <span class="md-ellipsis">
      Why GLMs?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-metrics-for-evaluating-regression-models" class="md-nav__link">
    <span class="md-ellipsis">
      Key Metrics for Evaluating Regression Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-dataset-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      A. Dataset Improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-improve-model-complexity-vc-dimension-bias-variance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vc-dimension-vapnik-chervonenkis-dimension" class="md-nav__link">
    <span class="md-ellipsis">
      VC Dimension (Vapnik-Chervonenkis Dimension)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-ways-to-measure-model-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      Other Ways to Measure Model Complexity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-regularization-avoid-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      C. Regularization (Avoid Overfitting)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C. Regularization (Avoid Overfitting)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regressionl2-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge regression(L2 regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution-bayesian-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution (Bayesian Interpretation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regressionl1-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Lasso Regression(L1 Regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_1" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-l1-regularization-perform-feature-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Why Does L1 Regularization Perform Feature Selection?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-net-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_2" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Intuition:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution_1" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-feature-engineering-selection" class="md-nav__link">
    <span class="md-ellipsis">
      D. Feature Engineering &amp; Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      E. Dimensionality Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-reduce-outliers-noise" class="md-nav__link">
    <span class="md-ellipsis">
      F. Reduce Outliers &amp; Noise
    </span>
  </a>
  
    <nav class="md-nav" aria-label="F. Reduce Outliers & Noise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intuition_3" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g-optimize-hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      G. Optimize Hyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="G. Optimize Hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grid-search" class="md-nav__link">
    <span class="md-ellipsis">
      Grid Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-search" class="md-nav__link">
    <span class="md-ellipsis">
      Random Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Optimization (BO)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Optimization (BO)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works_1" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of BO:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Components of BO:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-bayesian-optimization-is-powerful" class="md-nav__link">
    <span class="md-ellipsis">
      Why Bayesian Optimization is Powerful?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-of-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-ensemble-methods" class="md-nav__link">
    <span class="md-ellipsis">
      (6) Ensemble Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="(6) Ensemble Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Forest
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gbdt-gradient-boosted-decision-trees" class="md-nav__link">
    <span class="md-ellipsis">
      GBDT (Gradient Boosted Decision Trees)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GBDT (Gradient Boosted Decision Trees)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-by-step-process" class="md-nav__link">
    <span class="md-ellipsis">
      Step-by-Step Process:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-gbdt-works-well" class="md-nav__link">
    <span class="md-ellipsis">
      Why GBDT Works Well
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xgboost-extreme-gradient-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      XGBoost (Extreme Gradient Boosting)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lightgbm-light-gradient-boosting-machine" class="md-nav__link">
    <span class="md-ellipsis">
      LightGBM (Light Gradient Boosting Machine)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i-model-uncertainty-bayesianprobabilistic-models" class="md-nav__link">
    <span class="md-ellipsis">
      I. Model Uncertainty (Bayesian/Probabilistic Models)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Model Uncertainty (Bayesian/Probabilistic Models)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mle-for-betabeta" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \beta\beta
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mle-for-sigma2sigma2" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \sigma^2\sigma^2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computational-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Computational Techniques:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gaussian-processes" class="md-nav__link">
    <span class="md-ellipsis">
      Gaussian Processes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#j-optimization-algorithms-improving-model-training" class="md-nav__link">
    <span class="md-ellipsis">
      J. Optimization Algorithms (Improving Model Training)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="J. Optimization Algorithms (Improving Model Training)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      Newton's Method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum-based-update" class="md-nav__link">
    <span class="md-ellipsis">
      Momentum-based Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-model-architecture-optimization-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      K. Model Architecture Optimization (Deep Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l-interpretability-explainability" class="md-nav__link">
    <span class="md-ellipsis">
      L. Interpretability &amp; Explainability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#daily-quote" class="md-nav__link">
    <span class="md-ellipsis">
      Daily Quote
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tag" class="md-nav__link">
    <span class="md-ellipsis">
      Tag
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    <span class="md-ellipsis">
      Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      1. Linear Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Linear Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-assumptions" class="md-nav__link">
    <span class="md-ellipsis">
      Key Assumptions:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-multiple-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      2. Multiple Linear Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-logistic-regression-for-classification" class="md-nav__link">
    <span class="md-ellipsis">
      3. Logistic Regression (For Classification)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-polynomial-regression" class="md-nav__link">
    <span class="md-ellipsis">
      4. Polynomial Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-generalized-linear-model-glm" class="md-nav__link">
    <span class="md-ellipsis">
      5. Generalized Linear Model (GLM)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Generalized Linear Model (GLM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#general-form-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      General Form of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-glm" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of GLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-glms" class="md-nav__link">
    <span class="md-ellipsis">
      Why GLMs?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-metrics-for-evaluating-regression-models" class="md-nav__link">
    <span class="md-ellipsis">
      Key Metrics for Evaluating Regression Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-dataset-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      A. Dataset Improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-improve-model-complexity-vc-dimension-bias-variance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vc-dimension-vapnik-chervonenkis-dimension" class="md-nav__link">
    <span class="md-ellipsis">
      VC Dimension (Vapnik-Chervonenkis Dimension)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-ways-to-measure-model-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      Other Ways to Measure Model Complexity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-regularization-avoid-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      C. Regularization (Avoid Overfitting)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C. Regularization (Avoid Overfitting)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ridge-regressionl2-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge regression(L2 regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution-bayesian-interpretation" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution (Bayesian Interpretation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lasso-regressionl1-regulation" class="md-nav__link">
    <span class="md-ellipsis">
      Lasso Regression(L1 Regulation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_1" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-l1-regularization-perform-feature-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Why Does L1 Regularization Perform Feature Selection?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-net-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition_2" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Intuition:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relation-to-prior-distribution_1" class="md-nav__link">
    <span class="md-ellipsis">
      Relation to Prior Distribution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-feature-engineering-selection" class="md-nav__link">
    <span class="md-ellipsis">
      D. Feature Engineering &amp; Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      E. Dimensionality Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-reduce-outliers-noise" class="md-nav__link">
    <span class="md-ellipsis">
      F. Reduce Outliers &amp; Noise
    </span>
  </a>
  
    <nav class="md-nav" aria-label="F. Reduce Outliers & Noise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intuition_3" class="md-nav__link">
    <span class="md-ellipsis">
      Intuition:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g-optimize-hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      G. Optimize Hyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="G. Optimize Hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grid-search" class="md-nav__link">
    <span class="md-ellipsis">
      Grid Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-search" class="md-nav__link">
    <span class="md-ellipsis">
      Random Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Optimization (BO)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Optimization (BO)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works_1" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-components-of-bo" class="md-nav__link">
    <span class="md-ellipsis">
      Key Components of BO:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Components of BO:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-bayesian-optimization-is-powerful" class="md-nav__link">
    <span class="md-ellipsis">
      Why Bayesian Optimization is Powerful?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-of-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-ensemble-methods" class="md-nav__link">
    <span class="md-ellipsis">
      (6) Ensemble Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="(6) Ensemble Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Forest
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gbdt-gradient-boosted-decision-trees" class="md-nav__link">
    <span class="md-ellipsis">
      GBDT (Gradient Boosted Decision Trees)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GBDT (Gradient Boosted Decision Trees)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-by-step-process" class="md-nav__link">
    <span class="md-ellipsis">
      Step-by-Step Process:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-gbdt-works-well" class="md-nav__link">
    <span class="md-ellipsis">
      Why GBDT Works Well
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xgboost-extreme-gradient-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      XGBoost (Extreme Gradient Boosting)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lightgbm-light-gradient-boosting-machine" class="md-nav__link">
    <span class="md-ellipsis">
      LightGBM (Light Gradient Boosting Machine)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i-model-uncertainty-bayesianprobabilistic-models" class="md-nav__link">
    <span class="md-ellipsis">
      I. Model Uncertainty (Bayesian/Probabilistic Models)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Model Uncertainty (Bayesian/Probabilistic Models)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayesian Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mle-for-betabeta" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \beta\beta
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mle-for-sigma2sigma2" class="md-nav__link">
    <span class="md-ellipsis">
      MLE for \sigma^2\sigma^2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computational-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Computational Techniques:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gaussian-processes" class="md-nav__link">
    <span class="md-ellipsis">
      Gaussian Processes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#j-optimization-algorithms-improving-model-training" class="md-nav__link">
    <span class="md-ellipsis">
      J. Optimization Algorithms (Improving Model Training)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="J. Optimization Algorithms (Improving Model Training)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      Newton's Method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum-based-update" class="md-nav__link">
    <span class="md-ellipsis">
      Momentum-based Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-model-architecture-optimization-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      K. Model Architecture Optimization (Deep Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l-interpretability-explainability" class="md-nav__link">
    <span class="md-ellipsis">
      L. Interpretability &amp; Explainability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="01-introduction">01. Introduction</h1>
<h2 id="daily-quote">Daily Quote</h2>
<blockquote>
<p>[!quote] When you are offended at any man's fault, turn to yourself and study your own failings. Then you will forget your anger.<br />
— Epictetus</p>
</blockquote>
<h2 id="tag">Tag</h2>
<hr />
<p>Machine learning (ML) is the <strong>scientific study</strong> of <strong>algorithms</strong> and <mark style="background: #FFB8EBA6;">statistical models</mark> that <mark style="background: #FF5582A6;">computer systems</mark> use to perform a specific task without using explicit instructions, relying on patterns and <mark style="background: #FFB86CA6;">inference</mark> instead. It is seen as a subset of <mark style="background: #BBFABBA6;">artificial intelligence</mark>. Machine learning algorithms build a <mark style="background: #ABF7F7A6;">mathematical model</mark> based on sample data, known as "<mark style="background: #ADCCFFA6;">training data</mark>", in order to make predictions or decisions without being explicitly programmed to perform the task.</p>
<p>The technique of using linear interpolation for tabulation was believed to be used by Babylonian astronomers and mathematicians in Seleucid Mesopotamia (last three centuries BC), and by the Greek astronomer and mathematician, Hipparchus (2nd century BC). A description of linear interpolation can be found in the Almagest (2nd century AD) by Ptolemy.<br />
<img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250218182124.png" /></p>
<hr />
<h2 id="regression">Regression</h2>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-0-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-0-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-0-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-0-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-0-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-0-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-0-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-0-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-0-10">10</a></span>
<span class="normal"><a href="#__codelineno-0-11">11</a></span>
<span class="normal"><a href="#__codelineno-0-12">12</a></span>
<span class="normal"><a href="#__codelineno-0-13">13</a></span>
<span class="normal"><a href="#__codelineno-0-14">14</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="kn">import</span> <span class="nn">numpy.random</span> <span class="k">as</span> <span class="nn">npr</span>
<a id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-0-3" name="__codelineno-0-3"></a><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<a id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<a id="__codelineno-0-5" name="__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6"></a><span class="c1"># first generate some random data</span>
<a id="__codelineno-0-7" name="__codelineno-0-7"></a><span class="n">X</span> <span class="o">=</span> <span class="n">npr</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8"></a><span class="n">true_w</span><span class="p">,</span> <span class="n">true_b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<a id="__codelineno-0-9" name="__codelineno-0-9"></a><span class="c1"># add some noise to the labels</span>
<a id="__codelineno-0-10" name="__codelineno-0-10"></a><span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">*</span><span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">npr</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>
<a id="__codelineno-0-11" name="__codelineno-0-11"></a>
<a id="__codelineno-0-12" name="__codelineno-0-12"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<a id="__codelineno-0-13" name="__codelineno-0-13"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<a id="__codelineno-0-14" name="__codelineno-0-14"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219150927.png" /></p>
<p>Regression is a fundamental concept in statistics and machine learning that models the relationship between a dependent variable (target) and one or more independent variables (features). It is widely used for <strong>prediction, inference, and understanding data trends</strong>.</p>
<hr />
<h3 id="1-linear-regression"><strong>1. Linear Regression</strong></h3>
<p>Linear regression assumes a <strong>linear relationship</strong> between the input <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> and output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>. The model is expressed as:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y = \beta_0 + \beta_1 X + \epsilon
</div>
<script type="math/tex; mode=display">
Y = \beta_0 + \beta_1 X + \epsilon
</script>
</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta_0</span><script type="math/tex">\beta_0</script></span> is the <strong>intercept</strong> (constant term).</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta_1</span><script type="math/tex">\beta_1</script></span> is the <strong>coefficient</strong> (slope).</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> is the <strong>error term</strong> (captures noise or randomness in data).</p>
</li>
</ul>
<h4 id="key-assumptions"><strong>Key Assumptions:</strong></h4>
<ol>
<li>
<p><strong>Linearity:</strong> The relationship between <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> is linear.</p>
</li>
<li>
<p><strong>Independence:</strong> Observations are independent.</p>
</li>
<li>
<p><strong>Homoscedasticity:</strong> The variance of errors remains constant across values of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>.</p>
</li>
<li>
<p><strong>Normality of Errors:</strong> The residuals (errors) follow a normal distribution.</p>
</li>
</ol>
<h4 id="how-it-works"><strong>How It Works:</strong></h4>
<p>Linear regression finds the <strong>best-fit line</strong> by minimizing the sum of squared errors, using <strong>Ordinary Least Squares (OLS)</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\beta} \sum (Y_i - \hat{Y}_i)^2
</div>
<script type="math/tex; mode=display">
\min_{\beta} \sum (Y_i - \hat{Y}_i)^2
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{Y}_i</span><script type="math/tex">\hat{Y}_i</script></span> is the predicted value.</p>
<hr />
<h3 id="2-multiple-linear-regression"><strong>2. Multiple Linear Regression</strong></h3>
<p>Extends linear regression to <strong>multiple features</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon
</div>
<script type="math/tex; mode=display">
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon
</script>
</div>
<p>It captures the influence of multiple factors on the target variable.</p>
<hr />
<h3 id="3-logistic-regression-for-classification"><strong>3. Logistic Regression (For Classification)</strong></h3>
<p>Despite the name, <strong>logistic regression is used for classification</strong>, not regression. It models the probability of a binary outcome:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
</div>
<script type="math/tex; mode=display">
P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
</script>
</div>
<ul>
<li>
<p>It uses the <strong>logit (sigmoid) function</strong> to map predictions between 0 and 1.</p>
</li>
<li>
<p><strong>Generalized linear model (GLM)</strong>: It assumes a <strong>Bernoulli distribution</strong> for <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>.</p>
</li>
</ul>
<hr />
<h3 id="4-polynomial-regression"><strong>4. Polynomial Regression</strong></h3>
<p>If a relationship is <strong>non-linear</strong>, we extend linear regression to include polynomial terms:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_n X^n + \epsilon
</div>
<script type="math/tex; mode=display">
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_n X^n + \epsilon
</script>
</div>
<ul>
<li>
<p>It models curves instead of straight lines.</p>
</li>
<li>
<p>The higher the polynomial degree, the better it fits, but <strong>risk of overfitting</strong> increases.</p>
</li>
</ul>
<hr />
<h3 id="5-generalized-linear-model-glm"><strong>5. Generalized Linear Model (GLM)</strong></h3>
<p>A <strong>Generalized Linear Model (GLM)</strong> extends ordinary linear regression by allowing for:</p>
<ol>
<li>
<p><strong>Different Distributions</strong>: The response variable <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> can follow different distributions (not just normal).</p>
</li>
<li>
<p><strong>Link Function</strong>: Instead of modeling <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> directly, GLMs use a function to relate the mean <span class="arithmatex"><span class="MathJax_Preview">E[Y]</span><script type="math/tex">E[Y]</script></span> to the linear predictor.</p>
</li>
</ol>
<h4 id="general-form-of-glm"><strong>General Form of GLM</strong></h4>
<p>For a response variable <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>, GLMs assume:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g(E[Y]) = X\beta
</div>
<script type="math/tex; mode=display">
g(E[Y]) = X\beta
</script>
</div>
<p>Where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is the matrix of input features.</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> are the coefficients to learn.</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">g(\cdot)</span><script type="math/tex">g(\cdot)</script></span> is the <strong>link function</strong>, which connects the expected value of <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> to a linear combination of inputs.</p>
</li>
</ul>
<h4 id="key-components-of-glm"><strong>Key Components of GLM</strong></h4>
<ol>
<li>
<p><strong>Exponential Family of Distributions</strong>: <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> comes from a family of distributions (Normal, Poisson, Binomial, etc.).</p>
</li>
<li>
<p><strong>Link Function</strong> <span class="arithmatex"><span class="MathJax_Preview">g(\cdot)</span><script type="math/tex">g(\cdot)</script></span>: Transforms <span class="arithmatex"><span class="MathJax_Preview">E[Y]</span><script type="math/tex">E[Y]</script></span> into a linear model.</p>
</li>
<li>
<p><strong>Linear Predictor</strong>: <span class="arithmatex"><span class="MathJax_Preview">X\beta</span><script type="math/tex">X\beta</script></span> remains a linear combination of inputs.</p>
</li>
</ol>
<h4 id="why-glms"><strong>Why GLMs?</strong></h4>
<ul>
<li>
<p>Generalizes <strong>linear regression</strong> to different distributions.</p>
</li>
<li>
<p>Uses <strong>Maximum Likelihood Estimation (MLE)</strong> instead of least squares.</p>
</li>
<li>
<p>More flexible than simple linear regression.</p>
</li>
</ul>
<hr />
<h3 id="key-metrics-for-evaluating-regression-models"><strong>Key Metrics for Evaluating Regression Models</strong></h3>
<p>To measure the performance of a regression model, we use:</p>
<ol>
<li>
<p><strong>Mean Squared Error (MSE):</strong> Measures average squared differences between actual and predicted values.</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
MSE = \frac{1}{n} \sum (Y_i - \hat{Y}_i)^2  
</div>
<script type="math/tex; mode=display">  
MSE = \frac{1}{n} \sum (Y_i - \hat{Y}_i)^2  
</script>
</div>
</li>
<li>
<p><strong>Root Mean Squared Error (RMSE):</strong> Square root of MSE, which brings errors to the same unit as the data.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
RMSE = \sqrt{MSE}
</div>
<script type="math/tex; mode=display">
RMSE = \sqrt{MSE}
</script>
</div>
</li>
<li>
<p><strong>Mean Absolute Error (MAE):</strong> Measures the average absolute differences.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
MAE = \frac{1}{n} \sum |Y_i - \hat{Y}_i|
</div>
<script type="math/tex; mode=display">
MAE = \frac{1}{n} \sum |Y_i - \hat{Y}_i|
</script>
</div>
</li>
<li>
<p><strong><span class="arithmatex"><span class="MathJax_Preview">R^2</span><script type="math/tex">R^2</script></span> (Coefficient of Determination):</strong> Explains the proportion of variance explained by the model.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
</div>
<script type="math/tex; mode=display">
R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
</script>
</div>
<hr />
</li>
</ol>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-1-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-1-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-1-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-1-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-1-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-1-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-1-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-1-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-1-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-1-10">10</a></span>
<span class="normal"><a href="#__codelineno-1-11">11</a></span>
<span class="normal"><a href="#__codelineno-1-12">12</a></span>
<span class="normal"><a href="#__codelineno-1-13">13</a></span>
<span class="normal"><a href="#__codelineno-1-14">14</a></span>
<span class="normal"><a href="#__codelineno-1-15">15</a></span>
<span class="normal"><a href="#__codelineno-1-16">16</a></span>
<span class="normal"><a href="#__codelineno-1-17">17</a></span>
<span class="normal"><a href="#__codelineno-1-18">18</a></span>
<span class="normal"><a href="#__codelineno-1-19">19</a></span>
<span class="normal"><a href="#__codelineno-1-20">20</a></span>
<span class="normal"><a href="#__codelineno-1-21">21</a></span>
<span class="normal"><a href="#__codelineno-1-22">22</a></span>
<span class="normal"><a href="#__codelineno-1-23">23</a></span>
<span class="normal"><a href="#__codelineno-1-24">24</a></span>
<span class="normal"><a href="#__codelineno-1-25">25</a></span>
<span class="normal"><a href="#__codelineno-1-26">26</a></span>
<span class="normal"><a href="#__codelineno-1-27">27</a></span>
<span class="normal"><a href="#__codelineno-1-28">28</a></span>
<span class="normal"><a href="#__codelineno-1-29">29</a></span>
<span class="normal"><a href="#__codelineno-1-30">30</a></span>
<span class="normal"><a href="#__codelineno-1-31">31</a></span>
<span class="normal"><a href="#__codelineno-1-32">32</a></span>
<span class="normal"><a href="#__codelineno-1-33">33</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="c1"># the linear model</span>
<a id="__codelineno-1-2" name="__codelineno-1-2"></a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-1-3" name="__codelineno-1-3"></a>    <span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">params</span>
<a id="__codelineno-1-4" name="__codelineno-1-4"></a>    <span class="k">return</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
<a id="__codelineno-1-5" name="__codelineno-1-5"></a>
<a id="__codelineno-1-6" name="__codelineno-1-6"></a><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
<a id="__codelineno-1-7" name="__codelineno-1-7"></a>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span>
<a id="__codelineno-1-8" name="__codelineno-1-8"></a>    <span class="n">pred</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-1-9" name="__codelineno-1-9"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<a id="__codelineno-1-10" name="__codelineno-1-10"></a>
<a id="__codelineno-1-11" name="__codelineno-1-11"></a><span class="c1"># gradient function</span>
<a id="__codelineno-1-12" name="__codelineno-1-12"></a><span class="n">loss_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<a id="__codelineno-1-13" name="__codelineno-1-13"></a>
<a id="__codelineno-1-14" name="__codelineno-1-14"></a><span class="n">iterations</span> <span class="o">=</span> <span class="mi">500</span>
<a id="__codelineno-1-15" name="__codelineno-1-15"></a><span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>
<a id="__codelineno-1-16" name="__codelineno-1-16"></a><span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<a id="__codelineno-1-17" name="__codelineno-1-17"></a><span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.</span> <span class="c1"># initial values for the parameters</span>
<a id="__codelineno-1-18" name="__codelineno-1-18"></a><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
<a id="__codelineno-1-19" name="__codelineno-1-19"></a>    <span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<a id="__codelineno-1-20" name="__codelineno-1-20"></a>    <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-1-21" name="__codelineno-1-21"></a>    <span class="c1"># compute gradient w.r.t model parameters</span>
<a id="__codelineno-1-22" name="__codelineno-1-22"></a>    <span class="n">params_grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-1-23" name="__codelineno-1-23"></a>    <span class="c1"># update parameters</span>
<a id="__codelineno-1-24" name="__codelineno-1-24"></a>    <span class="n">w</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-1-25" name="__codelineno-1-25"></a>    <span class="n">b</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-1-26" name="__codelineno-1-26"></a>    <span class="c1">#print(loss_)</span>
<a id="__codelineno-1-27" name="__codelineno-1-27"></a>
<a id="__codelineno-1-28" name="__codelineno-1-28"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss: </span><span class="si">{}</span><span class="s2">, params </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss_</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
<a id="__codelineno-1-29" name="__codelineno-1-29"></a>
<a id="__codelineno-1-30" name="__codelineno-1-30"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<a id="__codelineno-1-31" name="__codelineno-1-31"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<a id="__codelineno-1-32" name="__codelineno-1-32"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<a id="__codelineno-1-33" name="__codelineno-1-33"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>loss: 0.25392094254493713,<br />
params (Array(0.26151434, dtype=float32, weak_type=True), Array(1.8786527, dtype=float32, weak_type=True))</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219151017.png" /></p>
<h2 id="optimization">Optimization</h2>
<p>To <strong>reduce RMSE</strong>, we can use different strategies:</p>
<h3 id="a-dataset-improvements"><strong>A. Dataset Improvements</strong></h3>
<ul>
<li>
<p><strong>More Data</strong>: Collect additional samples to improve generalization.</p>
</li>
<li>
<p><strong>Data Augmentation</strong>: Create synthetic data using transformations (e.g., image rotations, SMOTE for imbalanced data).</p>
</li>
<li>
<p><strong>Synthetic Data Generation</strong>: GANs, Variational Autoencoders, or bootstrapping.</p>
</li>
<li>
<p><strong>Handling Missing Data</strong>: Imputation methods (mean, median, KNN imputation).</p>
</li>
<li>
<p><strong>Class Balancing</strong>: Oversampling, undersampling, or synthetic data generation (e.g., SMOTE for imbalanced datasets).</p>
</li>
</ul>
<h3 id="b-improve-model-complexity-vc-dimension-bias-variance-tradeoff"><strong>B. Improve Model Complexity (VC Dimension, Bias-Variance Tradeoff)</strong></h3>
<ul>
<li>
<p><strong>Choose the Right Model</strong>: Simple models reduce overfitting; complex models improve accuracy but need regularization.</p>
</li>
<li>
<p><strong>Polynomial Features</strong>: Increase model complexity for non-linearity.</p>
</li>
<li>
<p><strong>Early Stopping</strong>: Stop training when validation loss starts increasing.</p>
</li>
</ul>
<h4 id="vc-dimension-vapnik-chervonenkis-dimension"><strong>VC Dimension (Vapnik-Chervonenkis Dimension)</strong></h4>
<p>VC dimension is a fundamental concept in statistical learning theory that measures the <strong>capacity</strong> or <strong>complexity</strong> of a model class. It quantifies how well a model can <strong>shatter</strong> (perfectly classify) different sets of data points.</p>
<blockquote>
<p>[!Definition]<br />
The <strong>VC dimension</strong> of a hypothesis class <span class="arithmatex"><span class="MathJax_Preview">\mathcal{H}</span><script type="math/tex">\mathcal{H}</script></span> is the largest number of points that can be <strong>shattered</strong> by some hypothesis in <span class="arithmatex"><span class="MathJax_Preview">\mathcal{H}</span><script type="math/tex">\mathcal{H}</script></span>.</p>
</blockquote>
<ul>
<li>
<p>A model <strong>shatters</strong> a dataset if it can classify all possible labelings (all <span class="arithmatex"><span class="MathJax_Preview">2^n</span><script type="math/tex">2^n</script></span> assignments of class labels for <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> points).</p>
</li>
<li>
<p>Higher VC dimension → More complex model → Higher capacity to fit data (risk of overfitting).</p>
</li>
<li>
<p>Example:</p>
<ul>
<li>
<p>A linear classifier in 2D can <strong>shatter</strong> at most <strong>3</strong> points, so its VC dimension is 3.</p>
</li>
<li>
<p>A linear classifier in <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>-dimensions has VC dimension <span class="arithmatex"><span class="MathJax_Preview">d+1</span><script type="math/tex">d+1</script></span>.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Implications:</strong></p>
<ul>
<li>
<p><strong>Lower VC dimension</strong> → Model is simple (underfitting risk).</p>
</li>
<li>
<p><strong>Higher VC dimension</strong> → Model is complex (overfitting risk).</p>
</li>
<li>
<p><strong>VC Theorem</strong>: If a model has a finite VC dimension <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>, it generalizes well if the number of training examples <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> is sufficiently larger than <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>.</p>
</li>
</ul>
<hr />
<h4 id="other-ways-to-measure-model-complexity"><strong>Other Ways to Measure Model Complexity</strong></h4>
<ol>
<li>
<p><strong>Rademacher Complexity</strong></p>
<ul>
<li>
<p>Measures how well a hypothesis class can fit random noise.</p>
</li>
<li>
<p>If a model class has high Rademacher complexity, it can overfit random labels.</p>
</li>
</ul>
</li>
<li>
<p><strong>PAC-Bayes</strong>, </p>
</li>
<li>
<p><strong>margin-based bounds</strong>  </p>
</li>
<li>
<p><strong>Geometric way</strong></p>
</li>
</ol>
<hr />
<ul>
<li>
<p>If the model is <strong>too simple</strong>, it underfits. We can:</p>
</li>
<li>
<p>Use polynomial regression instead of linear.</p>
</li>
<li>
<p>Add interaction terms.</p>
</li>
<li>
<p>Use <strong>nonlinear models</strong> (e.g., random forests, neural networks).
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-2-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-2-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-2-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-2-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-2-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-2-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-2-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-2-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-2-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-2-10">10</a></span>
<span class="normal"><a href="#__codelineno-2-11">11</a></span>
<span class="normal"><a href="#__codelineno-2-12">12</a></span>
<span class="normal"><a href="#__codelineno-2-13">13</a></span>
<span class="normal"><a href="#__codelineno-2-14">14</a></span>
<span class="normal"><a href="#__codelineno-2-15">15</a></span>
<span class="normal"><a href="#__codelineno-2-16">16</a></span>
<span class="normal"><a href="#__codelineno-2-17">17</a></span>
<span class="normal"><a href="#__codelineno-2-18">18</a></span>
<span class="normal"><a href="#__codelineno-2-19">19</a></span>
<span class="normal"><a href="#__codelineno-2-20">20</a></span>
<span class="normal"><a href="#__codelineno-2-21">21</a></span>
<span class="normal"><a href="#__codelineno-2-22">22</a></span>
<span class="normal"><a href="#__codelineno-2-23">23</a></span>
<span class="normal"><a href="#__codelineno-2-24">24</a></span>
<span class="normal"><a href="#__codelineno-2-25">25</a></span>
<span class="normal"><a href="#__codelineno-2-26">26</a></span>
<span class="normal"><a href="#__codelineno-2-27">27</a></span>
<span class="normal"><a href="#__codelineno-2-28">28</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="c1"># the nonlinear model</span>
<a id="__codelineno-2-2" name="__codelineno-2-2"></a><span class="k">def</span> <span class="nf">nonlinear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-2-3" name="__codelineno-2-3"></a>    <span class="n">res</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-2-4" name="__codelineno-2-4"></a>    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-2-5" name="__codelineno-2-5"></a>        <span class="n">res</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="n">idx</span>
<a id="__codelineno-2-6" name="__codelineno-2-6"></a>    <span class="k">return</span> <span class="n">res</span>
<a id="__codelineno-2-7" name="__codelineno-2-7"></a>
<a id="__codelineno-2-8" name="__codelineno-2-8"></a><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
<a id="__codelineno-2-9" name="__codelineno-2-9"></a>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span>
<a id="__codelineno-2-10" name="__codelineno-2-10"></a>    <span class="n">pred</span> <span class="o">=</span> <span class="n">nonlinear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-2-11" name="__codelineno-2-11"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<a id="__codelineno-2-12" name="__codelineno-2-12"></a>
<a id="__codelineno-2-13" name="__codelineno-2-13"></a><span class="c1"># gradient function</span>
<a id="__codelineno-2-14" name="__codelineno-2-14"></a><span class="n">loss_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<a id="__codelineno-2-15" name="__codelineno-2-15"></a>
<a id="__codelineno-2-16" name="__codelineno-2-16"></a><span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<a id="__codelineno-2-17" name="__codelineno-2-17"></a><span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
<a id="__codelineno-2-18" name="__codelineno-2-18"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">order</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span> 
<a id="__codelineno-2-19" name="__codelineno-2-19"></a>
<a id="__codelineno-2-20" name="__codelineno-2-20"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
<a id="__codelineno-2-21" name="__codelineno-2-21"></a>        <span class="n">params</span> <span class="o">=</span> <span class="n">weights</span>
<a id="__codelineno-2-22" name="__codelineno-2-22"></a>        <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-2-23" name="__codelineno-2-23"></a>        <span class="n">params_grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-2-24" name="__codelineno-2-24"></a>        <span class="c1"># update parameters</span>
<a id="__codelineno-2-25" name="__codelineno-2-25"></a>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">):</span>
<a id="__codelineno-2-26" name="__codelineno-2-26"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
<a id="__codelineno-2-27" name="__codelineno-2-27"></a>
<a id="__codelineno-2-28" name="__codelineno-2-28"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Order: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">loss_</span><span class="p">))</span>
</code></pre></div></td></tr></table></div></p>
</li>
</ul>
<blockquote>
<p>Order: 2, loss: 0.25392<br />
Order: 4, loss: 0.24112<br />
Order: 6, loss: 0.19777<br />
Order: 8, loss: 0.16633<br />
Order: 10, loss: 0.15277<br />
Order: 12, loss: 0.15003<br />
Order: 14, loss: 0.15189</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154334.png" /></p>
<h3 id="c-regularization-avoid-overfitting"><strong>C. Regularization (Avoid Overfitting)</strong></h3>
<ul>
<li>
<p><strong>L1 Regularization (Lasso <span class="arithmatex"><span class="MathJax_Preview">\ell_1</span><script type="math/tex">\ell_1</script></span>)</strong>: Encourages sparsity (feature selection).</p>
</li>
<li>
<p><strong>L2 Regularization (<span class="arithmatex"><span class="MathJax_Preview">\ell_2</span><script type="math/tex">\ell_2</script></span> Ridge)</strong>: Reduces large weights, preventing overfitting.</p>
</li>
<li>
<p><strong>Elastic Net</strong>: Combination of L1 and L2.</p>
</li>
<li>
<p><strong>Dropout (Neural Networks)</strong>: Randomly removes neurons during training.</p>
</li>
<li>
<p><strong>Batch Normalization</strong>: Normalizes activations in deep networks.</p>
</li>
</ul>
<hr />
<h4 id="ridge-regressionl2-regulation">Ridge regression(L2 regulation)</h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-3-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-3-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-3-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-3-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-3-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-3-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-3-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-3-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-3-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-3-10">10</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1"></a><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<a id="__codelineno-3-2" name="__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3"></a><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<a id="__codelineno-3-4" name="__codelineno-3-4"></a>
<a id="__codelineno-3-5" name="__codelineno-3-5"></a><span class="c1"># encoder</span>
<a id="__codelineno-3-6" name="__codelineno-3-6"></a><span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-3-7" name="__codelineno-3-7"></a>
<a id="__codelineno-3-8" name="__codelineno-3-8"></a><span class="c1"># ridge regression</span>
<a id="__codelineno-3-9" name="__codelineno-3-9"></a><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
<a id="__codelineno-3-10" name="__codelineno-3-10"></a><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.04097932204604149</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154350.png" /></p>
<p><strong>Ridge regression</strong> is a type of <strong>linear regression</strong> that includes an <span class="arithmatex"><span class="MathJax_Preview">L_2</span><script type="math/tex">L_2</script></span>-norm penalty to prevent overfitting. The objective function is:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
</div>
<script type="math/tex; mode=display">
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
</script>
</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> are the observed values,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">X_i</span><script type="math/tex">X_i</script></span> are the input features,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> are the regression coefficients,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> is the <strong>regularization parameter</strong>, which controls the trade-off between fitting the data and keeping coefficients small.</p>
</li>
</ul>
<h4 id="intuition"><strong>Intuition:</strong></h4>
<ul>
<li>
<p>Adding <span class="arithmatex"><span class="MathJax_Preview">\lambda \sum \beta_j^2</span><script type="math/tex">\lambda \sum \beta_j^2</script></span> <strong>shrinks</strong> the coefficients <span class="arithmatex"><span class="MathJax_Preview">\beta_j</span><script type="math/tex">\beta_j</script></span> towards zero but <strong>never exactly zero</strong>.</p>
</li>
<li>
<p>Ridge regression helps when features are <strong>highly correlated</strong>, reducing variance and preventing overfitting.</p>
</li>
</ul>
<h4 id="relation-to-prior-distribution-bayesian-interpretation"><strong>Relation to Prior Distribution (Bayesian Interpretation)</strong></h4>
<p>Ridge regression can be seen as <strong>Bayesian linear regression</strong> with a <strong>Gaussian prior</strong> on the coefficients:</p>
<p>$$</p>
<p>\beta_j \sim \mathcal{N}(0, \tau^2)</p>
<p>$$</p>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\tau^2 = \frac{1}{\lambda}</span><script type="math/tex">\tau^2 = \frac{1}{\lambda}</script></span>. This means:</p>
<ul>
<li>
<p>The model assumes that <strong>larger coefficients are less likely</strong>, which enforces shrinkage.</p>
</li>
<li>
<p>A <strong>stronger prior (larger <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>)</strong> shrinks the coefficients more.</p>
</li>
</ul>
<hr />
<h4 id="lasso-regressionl1-regulation">Lasso Regression(L1 Regulation)</h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-4-1">1</a></span>
<span class="normal"><a href="#__codelineno-4-2">2</a></span>
<span class="normal"><a href="#__codelineno-4-3">3</a></span>
<span class="normal"><a href="#__codelineno-4-4">4</a></span>
<span class="normal"><a href="#__codelineno-4-5">5</a></span>
<span class="normal"><a href="#__codelineno-4-6">6</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1"></a><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
<a id="__codelineno-4-2" name="__codelineno-4-2"></a><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
<a id="__codelineno-4-3" name="__codelineno-4-3"></a>
<a id="__codelineno-4-4" name="__codelineno-4-4"></a><span class="n">predictions</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<a id="__codelineno-4-5" name="__codelineno-4-5"></a><span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-4-6" name="__codelineno-4-6"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Loss): </span><span class="si">{</span><span class="n">mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.043433766812086105</p>
</blockquote>
<p><strong>Lasso (Least Absolute Shrinkage and Selection Operator)</strong> is another <strong>regularized regression</strong> method, but it uses an <span class="arithmatex"><span class="MathJax_Preview">L_1</span><script type="math/tex">L_1</script></span>-norm penalty instead of <span class="arithmatex"><span class="MathJax_Preview">L_2</span><script type="math/tex">L_2</script></span>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
</div>
<script type="math/tex; mode=display">
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
</script>
</div>
<h4 id="intuition_1"><strong>Intuition:</strong></h4>
<ul>
<li>
<p>The <strong>absolute value</strong> penalty forces some coefficients to be exactly <strong>zero</strong>, effectively performing <strong>feature selection</strong>.</p>
</li>
<li>
<p>Unlike Ridge regression, Lasso can create <strong>sparse models</strong>, making it useful when there are many irrelevant features.</p>
</li>
</ul>
<h4 id="relation-to-prior-distribution"><strong>Relation to Prior Distribution</strong></h4>
<p>Lasso regression corresponds to <strong>Bayesian regression</strong> with a <strong>Laplace prior</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_j \sim \text{Laplace}(0, b)
</div>
<script type="math/tex; mode=display">
\beta_j \sim \text{Laplace}(0, b)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">b = \frac{1}{\lambda}</span><script type="math/tex">b = \frac{1}{\lambda}</script></span>. The <strong>Laplace distribution</strong> is sharply peaked at zero, which encourages sparsity. This is why Lasso forces some coefficients to be exactly <strong>zero</strong>.</p>
<h4 id="why-does-l1-regularization-perform-feature-selection"><strong>Why Does L1 Regularization Perform Feature Selection?</strong></h4>
<ol>
<li>
<p><strong>Sparsity Effect:</strong></p>
<ul>
<li>
<p>The L1 penalty encourages some coefficients <span class="arithmatex"><span class="MathJax_Preview">\beta_j</span><script type="math/tex">\beta_j</script></span> to be exactly <strong>zero</strong>.</p>
</li>
<li>
<p>This leads to a <strong>sparse</strong> model, where only a subset of the features are retained.</p>
</li>
<li>
<p>Features with zero coefficients are effectively removed from the model.</p>
</li>
</ul>
</li>
<li>
<p><strong>Optimization Property:</strong></p>
<ul>
<li>
<p>The L1 norm creates a <strong>non-differentiable</strong> point at <span class="arithmatex"><span class="MathJax_Preview">\beta_j = 0</span><script type="math/tex">\beta_j = 0</script></span>, which forces some coefficients to shrink to zero.</p>
</li>
<li>
<p>Geometrically, the constraint region forms a <strong>diamond shape</strong>, which makes it more likely for the optimal solution to lie on the axes (i.e., some coefficients are zero).</p>
</li>
</ul>
</li>
<li>
<p><strong>Automatic Feature Selection:</strong></p>
<ul>
<li>
<p>Unlike L2 regularization (<strong>Ridge regression</strong>), which only shrinks coefficients towards zero, L1 <strong>eliminates</strong> irrelevant features entirely.</p>
</li>
<li>
<p>This is useful when you have <strong>high-dimensional data</strong> with many irrelevant or redundant features.</p>
</li>
</ul>
</li>
</ol>
<hr />
<h3 id="elastic-net-regression">Elastic Net Regression</h3>
<p><strong>Elastic Net</strong> combines both Ridge (<span class="arithmatex"><span class="MathJax_Preview">L_2</span><script type="math/tex">L_2</script></span>) and Lasso (<span class="arithmatex"><span class="MathJax_Preview">L_1</span><script type="math/tex">L_1</script></span>) penalties:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
</div>
<script type="math/tex; mode=display">
\min_{\beta} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
</script>
</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\lambda_1</span><script type="math/tex">\lambda_1</script></span> controls the Lasso part (feature selection).</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\lambda_2</span><script type="math/tex">\lambda_2</script></span> controls the Ridge part (shrinkage and handling collinearity).</p>
</li>
</ul>
<h3 id="intuition_2"><strong>Intuition:</strong></h3>
<ul>
<li>
<p>If features are <strong>highly correlated</strong>, Lasso can randomly pick one and discard the others. Elastic Net avoids this issue by keeping a mix of features.</p>
</li>
<li>
<p>It <strong>selects features like Lasso</strong> while <strong>shrinking coefficients like Ridge</strong>.</p>
</li>
</ul>
<h4 id="relation-to-prior-distribution_1"><strong>Relation to Prior Distribution</strong></h4>
<p>Elastic Net corresponds to a <strong>Mixture Prior</strong>:</p>
<ul>
<li>
<p>It assumes a <strong>combination</strong> of <strong>Gaussian (Ridge) and Laplace (Lasso) priors</strong> on the coefficients.</p>
</li>
<li>
<p>This means that some coefficients get <strong>shrunk</strong> (like Ridge), while others get <strong>sparsified</strong> (like Lasso).</p>
</li>
</ul>
<hr />
<h3 id="d-feature-engineering-selection"><strong>D. Feature Engineering &amp; Selection</strong></h3>
<ul>
<li>
<p><strong>One-hot Encoding</strong>: Convert categorical variables into numerical.</p>
</li>
<li>
<p><strong>Standardization (Z-score, Min-Max Scaling)</strong>: Normalize feature values.</p>
</li>
<li>
<p>Other <strong>feature transformations</strong> (log, power, etc.).</p>
</li>
<li>
<p><strong>Feature Selection Methods</strong>:</p>
<ul>
<li>
<p><strong>Univariate Tests</strong>: Select top-ranked features based on correlation.</p>
</li>
<li>
<p><strong>Recursive Feature Elimination (RFE)</strong>: Iteratively remove less important features.</p>
</li>
<li><strong>Lasso (L1 Regularization)</strong>: Selects only important features.</li>
<li>Remove <strong>irrelevant or redundant features</strong> to reduce noise.</li>
</ul>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-5-1">1</a></span>
<span class="normal"><a href="#__codelineno-5-2">2</a></span>
<span class="normal"><a href="#__codelineno-5-3">3</a></span>
<span class="normal"><a href="#__codelineno-5-4">4</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1"></a><span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
<a id="__codelineno-5-2" name="__codelineno-5-2"></a>
<a id="__codelineno-5-3" name="__codelineno-5-3"></a><span class="c1"># omit repeated code</span>
<a id="__codelineno-5-4" name="__codelineno-5-4"></a><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">))),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>loss: 0.07207518070936203</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219154941.png" /></p>
<ul>
<li>Perform <strong>dimensionality reduction</strong> (PCA, autoencoders).</li>
</ul>
<h3 id="e-dimensionality-reduction"><strong>E. Dimensionality Reduction</strong></h3>
<ul>
<li>
<p><strong>PCA (Principal Component Analysis)</strong>: Reduces correlated features.</p>
</li>
<li>
<p><strong>LDA (Linear Discriminant Analysis)</strong>: Used in classification tasks.</p>
</li>
<li>
<p><strong>t-SNE &amp; UMAP</strong>: Nonlinear dimensionality reduction for visualization.</p>
</li>
</ul>
<h3 id="f-reduce-outliers-noise"><strong>F. Reduce Outliers &amp; Noise</strong></h3>
<ul>
<li>
<p><strong>Z-score, IQR Method</strong>: Remove extreme outliers.</p>
</li>
<li>
<p><strong>Robust Loss Functions</strong>: Huber loss, quantile loss.</p>
</li>
<li>
<p><strong>Denoising Methods</strong>: Gaussian smoothing, moving average.</p>
<ul>
<li>RMSE is sensitive to <strong>outliers</strong>. </li>
</ul>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-6-1">1</a></span>
<span class="normal"><a href="#__codelineno-6-2">2</a></span>
<span class="normal"><a href="#__codelineno-6-3">3</a></span>
<span class="normal"><a href="#__codelineno-6-4">4</a></span>
<span class="normal"><a href="#__codelineno-6-5">5</a></span>
<span class="normal"><a href="#__codelineno-6-6">6</a></span>
<span class="normal"><a href="#__codelineno-6-7">7</a></span>
<span class="normal"><a href="#__codelineno-6-8">8</a></span>
<span class="normal"><a href="#__codelineno-6-9">9</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>
<a id="__codelineno-6-2" name="__codelineno-6-2"></a>
<a id="__codelineno-6-3" name="__codelineno-6-3"></a><span class="n">iso_forest</span> <span class="o">=</span> <span class="n">IsolationForest</span><span class="p">(</span><span class="n">contamination</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>  
<a id="__codelineno-6-4" name="__codelineno-6-4"></a><span class="n">outliers</span> <span class="o">=</span> <span class="n">iso_forest</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<a id="__codelineno-6-5" name="__codelineno-6-5"></a>
<a id="__codelineno-6-6" name="__codelineno-6-6"></a><span class="n">inliers</span> <span class="o">=</span> <span class="n">outliers</span> <span class="o">==</span> <span class="mi">1</span>
<a id="__codelineno-6-7" name="__codelineno-6-7"></a>
<a id="__codelineno-6-8" name="__codelineno-6-8"></a><span class="n">X_filtered</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inliers</span><span class="p">]</span>
<a id="__codelineno-6-9" name="__codelineno-6-9"></a><span class="n">Y_filtered</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">inliers</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.11134050786495209</p>
</blockquote>
<ul>
<li>Robust regression (Huber loss).
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-7-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1"></a><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">HuberRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
</code></pre></div></td></tr></table></div><blockquote>
<p>Mean Squared Error (Loss): 0.03720296174287796</p>
</blockquote>
</li>
</ul>
<p>The <strong>Huber loss</strong> is a combination of <strong>squared loss</strong> (used in ordinary least squares) and <strong>absolute loss</strong> (used in robust regression), designed to handle <strong>outliers</strong> more gracefully. It is particularly useful when you want a model that is robust to <strong>outliers</strong> but still penalizes small errors quadratically, like the regular least squares.</p>
<blockquote>
<p>[!Definition] Definition<br />
The Huber loss function <span class="arithmatex"><span class="MathJax_Preview">L_\delta(y, \hat{y})</span><script type="math/tex">L_\delta(y, \hat{y})</script></span> is defined as:
$$
L_\delta(y, \hat{y}) =<br />
\begin{cases}<br />
\frac{1}{2}(y - \hat{y})^2, &amp; \text{for} \ |y - \hat{y}| \leq \delta \<br />
\delta |y - \hat{y}| - \frac{1}{2} \delta^2, &amp; \text{for} \ |y - \hat{y}| &gt; \delta<br />
\end{cases}
$$
where:
- <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is the true value,
- <span class="arithmatex"><span class="MathJax_Preview">\hat{y}</span><script type="math/tex">\hat{y}</script></span> is the predicted value,
- <span class="arithmatex"><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> is a threshold that controls the transition between quadratic and linear loss.</p>
</blockquote>
<h4 id="intuition_3"><strong>Intuition:</strong></h4>
<ul>
<li>
<p>For errors less than or equal to <span class="arithmatex"><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>, the loss behaves like a <strong>squared error</strong> (quadratic), which heavily penalizes small deviations.</p>
</li>
<li>
<p>For large errors (i.e., when the error is greater than <span class="arithmatex"><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>), the loss behaves like <strong>absolute error</strong>, which grows linearly, avoiding excessive penalties for large deviations or outliers.</p>
</li>
</ul>
<p>This makes the <strong>Huber loss</strong> more <strong>robust to outliers</strong> compared to squared error (which can be highly influenced by large deviations).</p>
<h3 id="g-optimize-hyperparameters"><strong>G. Optimize Hyperparameters</strong></h3>
<ul>
<li>
<p><strong>Grid Search</strong>: Exhaustively tries all combinations.</p>
</li>
<li>
<p><strong>Random Search</strong>: Randomly samples hyperparameters.</p>
</li>
<li>
<p><strong>Bayesian Optimization</strong>: Uses probability to find optimal hyperparameters.</p>
</li>
<li>
<p><strong>Evolutionary Algorithms</strong>: Genetic algorithms to find the best configuration.</p>
</li>
</ul>
<h4 id="grid-search"><strong>Grid Search</strong></h4>
<ul>
<li>
<p>Exhaustively searches over a predefined set of hyperparameters.</p>
</li>
<li>
<p>Creates a <strong>grid</strong> of possible values and evaluates each combination.</p>
</li>
</ul>
<p><strong>Example:</strong></p>
<p>If we have:</p>
<ul>
<li>
<p>Learning rate <span class="arithmatex"><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> in {0.001, 0.01, 0.1}</p>
</li>
<li>
<p>Regularization <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> in {0.001, 0.01, 0.1}</p>
</li>
</ul>
<p>Grid search tests all <span class="arithmatex"><span class="MathJax_Preview">3 \times 3 = 9</span><script type="math/tex">3 \times 3 = 9</script></span> combinations.</p>
<p>✅ <strong>Pros:</strong></p>
<ul>
<li>
<p>Guarantees best result within search space.</p>
</li>
<li>
<p>Simple and systematic.</p>
</li>
</ul>
<p>❌ <strong>Cons:</strong></p>
<ul>
<li>
<p>Computationally <strong>expensive</strong>.</p>
</li>
<li>
<p>Wasteful if only a few hyperparameters matter.</p>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-8-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-8-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-8-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-8-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-8-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-8-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-8-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-8-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-8-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-8-10">10</a></span>
<span class="normal"><a href="#__codelineno-8-11">11</a></span>
<span class="normal"><a href="#__codelineno-8-12">12</a></span>
<span class="normal"><a href="#__codelineno-8-13">13</a></span>
<span class="normal"><a href="#__codelineno-8-14">14</a></span>
<span class="normal"><a href="#__codelineno-8-15">15</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<a id="__codelineno-8-2" name="__codelineno-8-2"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<a id="__codelineno-8-3" name="__codelineno-8-3"></a>
<a id="__codelineno-8-4" name="__codelineno-8-4"></a><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
<a id="__codelineno-8-5" name="__codelineno-8-5"></a>    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
<a id="__codelineno-8-6" name="__codelineno-8-6"></a>    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<a id="__codelineno-8-7" name="__codelineno-8-7"></a>    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<a id="__codelineno-8-8" name="__codelineno-8-8"></a><span class="p">}</span>
<a id="__codelineno-8-9" name="__codelineno-8-9"></a>
<a id="__codelineno-8-10" name="__codelineno-8-10"></a><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<a id="__codelineno-8-11" name="__codelineno-8-11"></a><span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<a id="__codelineno-8-12" name="__codelineno-8-12"></a><span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-8-13" name="__codelineno-8-13"></a>
<a id="__codelineno-8-14" name="__codelineno-8-14"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<a id="__codelineno-8-15" name="__codelineno-8-15"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best score:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h4 id="random-search"><strong>Random Search</strong></h4>
<ul>
<li>
<p>Randomly samples hyperparameter values from a distribution.</p>
</li>
<li>
<p>Does not evaluate all combinations but instead explores the space <strong>stochastically</strong>.</p>
</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>Instead of testing all 9 combinations, randomly select 5 combinations.</li>
</ul>
<p>✅ <strong>Pros:</strong></p>
<ul>
<li>
<p>Often more <strong>efficient</strong> than grid search.</p>
</li>
<li>
<p>Works well when <strong>only a few hyperparameters significantly affect performance</strong>.</p>
</li>
<li>
<p><strong>More effective in high-dimensional spaces</strong>.</p>
</li>
</ul>
<p>❌ <strong>Cons:</strong>
- No guarantee of finding the absolute best hyperparameters.</p>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-9-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-9-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-9-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-9-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-9-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-9-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-9-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-9-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-9-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-9-10">10</a></span>
<span class="normal"><a href="#__codelineno-9-11">11</a></span>
<span class="normal"><a href="#__codelineno-9-12">12</a></span>
<span class="normal"><a href="#__codelineno-9-13">13</a></span>
<span class="normal"><a href="#__codelineno-9-14">14</a></span>
<span class="normal"><a href="#__codelineno-9-15">15</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<a id="__codelineno-9-2" name="__codelineno-9-2"></a><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span>
<a id="__codelineno-9-3" name="__codelineno-9-3"></a>
<a id="__codelineno-9-4" name="__codelineno-9-4"></a><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span>
<a id="__codelineno-9-5" name="__codelineno-9-5"></a>    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
<a id="__codelineno-9-6" name="__codelineno-9-6"></a>    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
<a id="__codelineno-9-7" name="__codelineno-9-7"></a>    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<a id="__codelineno-9-8" name="__codelineno-9-8"></a><span class="p">}</span>
<a id="__codelineno-9-9" name="__codelineno-9-9"></a>
<a id="__codelineno-9-10" name="__codelineno-9-10"></a><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<a id="__codelineno-9-11" name="__codelineno-9-11"></a><span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_dist</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-9-12" name="__codelineno-9-12"></a><span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-9-13" name="__codelineno-9-13"></a>
<a id="__codelineno-9-14" name="__codelineno-9-14"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:&quot;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<a id="__codelineno-9-15" name="__codelineno-9-15"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best score:&quot;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h4 id="bayesian-optimization-bo"><strong>Bayesian Optimization (BO)</strong></h4>
<p>Grid and random search <strong>do not use past evaluations</strong> to guide future searches. <strong>Bayesian Optimization (BO)</strong> does.</p>
<h5 id="how-it-works_1"><strong>How It Works</strong></h5>
<ol>
<li>
<p>Define a <strong>prior</strong> belief about the function mapping hyperparameters to performance.</p>
</li>
<li>
<p>Use a <strong>surrogate model</strong> (usually a <strong>Gaussian Process</strong>) to estimate the objective function.</p>
</li>
<li>
<p>Select the next hyperparameter set based on an <strong>acquisition function</strong> (e.g., Expected Improvement, Upper Confidence Bound).</p>
</li>
<li>
<p>Evaluate the model with these hyperparameters.</p>
</li>
<li>
<p>Update the belief and repeat.</p>
</li>
</ol>
<hr />
<h4 id="key-components-of-bo"><strong>Key Components of BO:</strong></h4>
<ul>
<li>
<p><strong>Surrogate Model:</strong>  </p>
<ul>
<li>
<p>Typically a <strong>Gaussian Process (GP)</strong> that models the unknown function.</p>
</li>
<li>
<p>Uses previous evaluations to estimate performance at new points.</p>
</li>
</ul>
</li>
<li>
<p><strong>Acquisition Function:</strong> </p>
<ul>
<li>Guides the search by balancing <strong>exploration</strong> (trying new areas) and <strong>exploitation</strong> (focusing on promising regions).</li>
</ul>
</li>
<li>
<p>Common choices:   </p>
<ul>
<li>
<p><strong>Expected Improvement (EI)</strong>: Pick points expected to improve the best result.</p>
</li>
<li>
<p><strong>Upper Confidence Bound (UCB)</strong>: Pick points with high uncertainty.</p>
</li>
</ul>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-10-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-10-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-10-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-10-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-10-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-10-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-10-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-10-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-10-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-10-10">10</a></span>
<span class="normal"><a href="#__codelineno-10-11">11</a></span>
<span class="normal"><a href="#__codelineno-10-12">12</a></span>
<span class="normal"><a href="#__codelineno-10-13">13</a></span>
<span class="normal"><a href="#__codelineno-10-14">14</a></span>
<span class="normal"><a href="#__codelineno-10-15">15</a></span>
<span class="normal"><a href="#__codelineno-10-16">16</a></span>
<span class="normal"><a href="#__codelineno-10-17">17</a></span>
<span class="normal"><a href="#__codelineno-10-18">18</a></span>
<span class="normal"><a href="#__codelineno-10-19">19</a></span>
<span class="normal"><a href="#__codelineno-10-20">20</a></span>
<span class="normal"><a href="#__codelineno-10-21">21</a></span>
<span class="normal"><a href="#__codelineno-10-22">22</a></span>
<span class="normal"><a href="#__codelineno-10-23">23</a></span>
<span class="normal"><a href="#__codelineno-10-24">24</a></span>
<span class="normal"><a href="#__codelineno-10-25">25</a></span>
<span class="normal"><a href="#__codelineno-10-26">26</a></span>
<span class="normal"><a href="#__codelineno-10-27">27</a></span>
<span class="normal"><a href="#__codelineno-10-28">28</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1"></a><span class="kn">from</span> <span class="nn">bayes_opt</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>
<a id="__codelineno-10-2" name="__codelineno-10-2"></a>
<a id="__codelineno-10-3" name="__codelineno-10-3"></a><span class="k">def</span> <span class="nf">rf_cv</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">):</span>
<a id="__codelineno-10-4" name="__codelineno-10-4"></a>    <span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
<a id="__codelineno-10-5" name="__codelineno-10-5"></a>        <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">),</span>
<a id="__codelineno-10-6" name="__codelineno-10-6"></a>        <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">max_depth</span><span class="p">),</span>
<a id="__codelineno-10-7" name="__codelineno-10-7"></a>        <span class="n">min_samples_split</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">min_samples_split</span><span class="p">),</span>
<a id="__codelineno-10-8" name="__codelineno-10-8"></a>        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-10-9" name="__codelineno-10-9"></a>    <span class="p">)</span>
<a id="__codelineno-10-10" name="__codelineno-10-10"></a>    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-10-11" name="__codelineno-10-11"></a>    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  <span class="c1"># Optimization objective</span>
<a id="__codelineno-10-12" name="__codelineno-10-12"></a>
<a id="__codelineno-10-13" name="__codelineno-10-13"></a><span class="c1"># Define hyperparameter space</span>
<a id="__codelineno-10-14" name="__codelineno-10-14"></a><span class="n">pbounds</span> <span class="o">=</span> <span class="p">{</span>
<a id="__codelineno-10-15" name="__codelineno-10-15"></a>    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
<a id="__codelineno-10-16" name="__codelineno-10-16"></a>    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
<a id="__codelineno-10-17" name="__codelineno-10-17"></a>    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<a id="__codelineno-10-18" name="__codelineno-10-18"></a><span class="p">}</span>
<a id="__codelineno-10-19" name="__codelineno-10-19"></a>
<a id="__codelineno-10-20" name="__codelineno-10-20"></a><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span>
<a id="__codelineno-10-21" name="__codelineno-10-21"></a>    <span class="n">f</span><span class="o">=</span><span class="n">rf_cv</span><span class="p">,</span>
<a id="__codelineno-10-22" name="__codelineno-10-22"></a>    <span class="n">pbounds</span><span class="o">=</span><span class="n">pbounds</span><span class="p">,</span>
<a id="__codelineno-10-23" name="__codelineno-10-23"></a>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<a id="__codelineno-10-24" name="__codelineno-10-24"></a><span class="p">)</span>
<a id="__codelineno-10-25" name="__codelineno-10-25"></a>
<a id="__codelineno-10-26" name="__codelineno-10-26"></a><span class="n">optimizer</span><span class="o">.</span><span class="n">maximize</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<a id="__codelineno-10-27" name="__codelineno-10-27"></a>
<a id="__codelineno-10-28" name="__codelineno-10-28"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found:&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<hr />
<h5 id="why-bayesian-optimization-is-powerful"><strong>Why Bayesian Optimization is Powerful?</strong></h5>
<p>✅ <strong>More efficient than grid/random search</strong> → Finds good hyperparameters in fewer evaluations.</p>
<p>✅ <strong>Works well for expensive models</strong> (e.g., deep learning, reinforcement learning).</p>
<p>✅ <strong>Uses prior knowledge</strong> to guide search intelligently.</p>
<p>❌ <strong>Downside?</strong></p>
<ul>
<li>
<p>Computationally <strong>expensive</strong> for very high-dimensional problems.</p>
</li>
<li>
<p>Assumes a smooth objective function, which may not always hold.</p>
</li>
</ul>
<hr />
<h4 id="comparison-of-methods"><strong>Comparison of Methods</strong></h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Grid Search</strong></td>
<td>Systematic, finds best result within search space</td>
<td>Expensive, infeasible in high dimensions</td>
</tr>
<tr>
<td><strong>Random Search</strong></td>
<td>More efficient than grid search, better exploration</td>
<td>No guarantee of best hyperparameters</td>
</tr>
<tr>
<td><strong>Bayesian Optimization</strong></td>
<td>Uses prior knowledge, efficient in low-data settings</td>
<td>Computationally expensive in high dimensions</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="6-ensemble-methods"><strong>(6) Ensemble Methods</strong></h3>
<ul>
<li>
<p><strong>Bagging</strong> (Bootstrap Aggregating): Train multiple models on bootstrapped datasets (e.g., Random Forest).</p>
</li>
<li>
<p><strong>Boosting</strong>: Sequential training that corrects previous errors (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM).</p>
</li>
<li>
<p><strong>Stacking</strong>: Combine multiple models via a meta-model.</p>
</li>
</ul>
<h4 id="random-forest">Random Forest</h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-11-1">1</a></span>
<span class="normal"><a href="#__codelineno-11-2">2</a></span>
<span class="normal"><a href="#__codelineno-11-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<a id="__codelineno-11-2" name="__codelineno-11-2"></a>
<a id="__codelineno-11-3" name="__codelineno-11-3"></a><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.008086484856903553</p>
</blockquote>
<p><img alt="" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219172219.png" /></p>
<p>Random Forest is an <strong>ensemble learning method</strong> that builds multiple decision trees and combines their outputs to improve prediction accuracy and reduce overfitting.</p>
<p><strong>How Random Forest Works</strong></p>
<ul>
<li>
<p><strong>Bootstrap Aggregation (Bagging):</strong></p>
</li>
<li>
<p>Random Forest trains multiple decision trees on different <strong>bootstrapped samples</strong> of the dataset.</p>
</li>
<li>
<p>Each tree is trained on a <strong>random subset</strong> (without replacement) of the features.</p>
</li>
<li>
<p>The predictions from all trees are <strong>averaged</strong> (for regression) or use <strong>majority voting</strong> (for classification).</p>
</li>
</ul>
<p><strong>Why it Works:</strong></p>
<ul>
<li>
<p>Reduces <strong>overfitting</strong> (compared to a single decision tree).</p>
</li>
<li>
<p>Handles <strong>high-dimensional data</strong> well.</p>
</li>
<li>
<p><strong>Robust to noise</strong> and missing values.</p>
</li>
</ul>
<p><strong>Pros &amp; Cons</strong></p>
<p>✅ Reduces variance (compared to individual trees).</p>
<p>✅ Handles categorical &amp; numerical features well.</p>
<p>✅ Works well for large datasets.</p>
<p>❌ Slower than a single decision tree.</p>
<p>❌ Large ensembles can be computationally expensive.</p>
<h4 id="gbdt-gradient-boosted-decision-trees">GBDT (Gradient Boosted Decision Trees)</h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-12-1">1</a></span>
<span class="normal"><a href="#__codelineno-12-2">2</a></span>
<span class="normal"><a href="#__codelineno-12-3">3</a></span>
<span class="normal"><a href="#__codelineno-12-4">4</a></span>
<span class="normal"><a href="#__codelineno-12-5">5</a></span>
<span class="normal"><a href="#__codelineno-12-6">6</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1"></a><span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
<a id="__codelineno-12-2" name="__codelineno-12-2"></a>
<a id="__codelineno-12-3" name="__codelineno-12-3"></a><span class="n">lgb_train</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<a id="__codelineno-12-4" name="__codelineno-12-4"></a><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;regression&#39;</span><span class="p">,</span> <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gbdt&#39;</span><span class="p">,</span> <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span>
<a id="__codelineno-12-5" name="__codelineno-12-5"></a><span class="n">lgb_model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lgb_train</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<a id="__codelineno-12-6" name="__codelineno-12-6"></a><span class="n">predictions</span> <span class="o">=</span> <span class="n">lgb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.19826513528823853</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219172346.png" /></p>
<p>GBDT (Gradient Boosted Decision Trees) is a <strong>powerful ensemble learning method</strong> that builds multiple decision trees sequentially to <strong>minimize a given loss function</strong> using gradient descent. It is widely used in machine learning for regression, classification, and ranking problems.</p>
<p><strong>How GBDT Works</strong></p>
<p>GBDT is based on the <strong>boosting</strong> technique, where models are trained <strong>sequentially</strong>, and each tree tries to correct the errors made by the previous ones.</p>
<h5 id="step-by-step-process"><strong>Step-by-Step Process:</strong></h5>
<ol>
<li>
<p><strong>Start with a weak model</strong></p>
<ul>
<li>
<p>Usually, this is a simple decision tree (often a shallow one, also called a weak learner).</p>
</li>
<li>
<p>The first tree predicts the target values roughly.</p>
</li>
</ul>
</li>
<li>
<p><strong>Compute the residual errors (gradients of the loss function)</strong></p>
<ul>
<li>For regression:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
r_i = y_i - \hat{y}_i
</div>
<script type="math/tex; mode=display">
r_i = y_i - \hat{y}_i
</script>
</div>
<ul>
<li>For classification:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
r_i = -\frac{\partial L(y, \hat{y})}{\partial \hat{y}}
</div>
<script type="math/tex; mode=display">
r_i = -\frac{\partial L(y, \hat{y})}{\partial \hat{y}}
</script>
</div>
<ul>
<li>The model learns the difference (residuals) between the predicted and actual values.</li>
</ul>
</li>
<li>
<p><strong>Train a new decision tree to predict the residuals</strong></p>
<ul>
<li>This new tree focuses on correcting the errors made by the previous one.</li>
</ul>
</li>
<li>
<p><strong>Update the prediction</strong></p>
<ul>
<li>Add the new tree's predictions to the overall model:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
F_{m+1}(x) = F_m(x) + \eta h_m(x)
</div>
<script type="math/tex; mode=display">
F_{m+1}(x) = F_m(x) + \eta h_m(x)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> is the <strong>learning rate</strong>, which controls how much each tree contributes to the final prediction.</li>
</ul>
</li>
<li>
<p><strong>Repeat the process</strong></p>
<ul>
<li>Keep adding new trees until a stopping criterion is met (e.g., a fixed number of trees or no significant improvement in performance).</li>
</ul>
</li>
</ol>
<h5 id="why-gbdt-works-well"><strong>Why GBDT Works Well</strong></h5>
<ul>
<li>
<p><strong>Handles Non-Linearity:</strong> Unlike linear models, GBDT captures complex, non-linear relationships.</p>
</li>
<li>
<p><strong>Adaptive Learning:</strong> Each new tree corrects errors made by previous ones.</p>
</li>
<li>
<p><strong>Robust to Outliers:</strong> GBDT can be robust if loss functions like Huber loss are used.</p>
</li>
<li>
<p><strong>Feature Importance:</strong> It naturally provides feature importance rankings.</p>
</li>
</ul>
<hr />
<h4 id="xgboost-extreme-gradient-boosting"><strong>XGBoost (Extreme Gradient Boosting)</strong></h4>
<p>XGBoost is an optimized implementation of <strong>gradient boosting</strong> that is highly efficient and widely used in machine learning competitions.</p>
<p><strong>How XGBoost Works</strong></p>
<ul>
<li>
<p><strong>Gradient Boosting:</strong></p>
<ul>
<li>
<p>Unlike Random Forest (which trains trees independently), XGBoost <strong>trains trees sequentially</strong>, where each new tree corrects the errors of the previous ones.</p>
</li>
<li>
<p>It <strong>minimizes a loss function</strong> using <strong>gradient descent</strong>, hence the name "gradient boosting."</p>
</li>
</ul>
</li>
<li>
<p><strong>Key Optimizations:</strong></p>
<ul>
<li>
<p><strong>Regularization:</strong> Adds L1/L2 penalties (like Lasso/Ridge) to prevent overfitting.</p>
</li>
<li>
<p><strong>Tree Pruning:</strong> Uses a <strong>maximum depth</strong> instead of splitting until pure.</p>
</li>
<li>
<p><strong>Handling Missing Values:</strong> Automatically learns best imputation.</p>
</li>
<li>
<p><strong>Parallelization:</strong> Efficiently builds trees using multi-threading.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Pros &amp; Cons</strong></p>
<p>✅ Faster than traditional gradient boosting (due to optimizations).</p>
<p>✅ Handles large-scale data efficiently.</p>
<p>✅ Highly customizable with tunable hyperparameters.</p>
<p>❌ Can be prone to overfitting if not tuned properly.</p>
<p>❌ Sensitive to hyperparameter settings.</p>
<hr />
<h4 id="lightgbm-light-gradient-boosting-machine"><strong>LightGBM (Light Gradient Boosting Machine)</strong></h4>
<p>LightGBM is another gradient boosting library, optimized for <strong>speed</strong> and <strong>efficiency</strong>.</p>
<p><strong>How LightGBM Works</strong></p>
<ul>
<li>
<p><strong>Key Differences from XGBoost:</strong></p>
</li>
<li>
<p><strong>Leaf-wise tree growth:</strong></p>
<ul>
<li>
<p>XGBoost grows trees level-wise (uniform expansion).</p>
</li>
<li>
<p>LightGBM grows trees leaf-wise (expands the most promising leaf first), leading to deeper trees and faster convergence.</p>
</li>
</ul>
</li>
<li>
<p><strong>Histogram-based splitting:</strong></p>
<ul>
<li>Instead of checking every data point for splits, LightGBM groups values into <strong>histogram bins</strong>, reducing computation.</li>
</ul>
</li>
<li>
<p><strong>Better memory efficiency:</strong></p>
<ul>
<li>Uses <strong>less RAM</strong> than XGBoost due to histogram-based processing.</li>
</ul>
</li>
</ul>
<p><strong>Pros &amp; Cons</strong></p>
<p>✅ Much <strong>faster</strong> than XGBoost, especially for large datasets.</p>
<p>✅ <strong>Handles categorical features</strong> without needing one-hot encoding.</p>
<p>✅ Scales well to large datasets.</p>
<p>❌ May be <strong>less interpretable</strong> than XGBoost.</p>
<p>❌ Can overfit if not properly tuned.</p>
<p><strong>When to Use Which?</strong></p>
<ul>
<li>
<p><strong>Random Forest</strong> → When you need an <strong>interpretable model</strong> and have <strong>limited data</strong>.</p>
</li>
<li>
<p><strong>XGBoost</strong> → When you need <strong>high accuracy</strong> and have time for <strong>hyperparameter tuning</strong>.</p>
</li>
<li>
<p><strong>LightGBM</strong> → When you have <strong>large datasets</strong> and need <strong>fast training</strong>.</p>
</li>
</ul>
<h3 id="i-model-uncertainty-bayesianprobabilistic-models"><strong>I. Model Uncertainty (Bayesian/Probabilistic Models)</strong></h3>
<ul>
<li>
<p><strong>Bayesian Neural Networks</strong>: Treats weights as probability distributions.</p>
</li>
<li>
<p><strong>Gaussian Processes</strong>: Models uncertainty in regression.</p>
</li>
<li>
<p><strong>Monte Carlo Dropout</strong>: Uses dropout at inference to approximate Bayesian uncertainty.</p>
</li>
</ul>
<h4 id="bayesian-regression"><strong>Bayesian Regression</strong></h4>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-13-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1"></a><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<blockquote>
<p>Mean Squared Error (Loss): 0.03729788959026337</p>
</blockquote>
<p><img alt="image.png" src="https://raw.githubusercontent.com/leon2milan/FigureBed/master/20250219161240.png" /></p>
<p><strong>Bayesian Regression</strong> is a probabilistic approach to linear regression. Instead of estimating a single set of parameters (as in ordinary least squares), it treats the parameters of the regression model as random variables with a prior distribution.</p>
<ol>
<li>
<p><strong>Model Assumptions</strong>:</p>
<ul>
<li>
<p>Assume a linear model for the data:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y = X\beta + \epsilon
</div>
<script type="math/tex; mode=display">
y = X\beta + \epsilon
</script>
</div>
<p>where:</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is the vector of observed values,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is the design matrix (input features),</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> is the vector of regression coefficients (parameters),</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\epsilon \sim \mathcal{N}(0, \sigma^2 I)</span><script type="math/tex">\epsilon \sim \mathcal{N}(0, \sigma^2 I)</script></span> is the Gaussian noise with variance <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>.</p>
</li>
</ul>
</li>
<li>
<p><strong>Prior</strong>:</p>
<ul>
<li>Assume a <strong>Gaussian prior</strong> on the coefficients <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p(\beta) = \mathcal{N}(\beta \mid 0, \tau^2 I)
</div>
<script type="math/tex; mode=display">
p(\beta) = \mathcal{N}(\beta \mid 0, \tau^2 I)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\tau^2</span><script type="math/tex">\tau^2</script></span> is the prior variance of the coefficients.</p>
</li>
<li>
<p><strong>Likelihood</strong>:</p>
<ul>
<li>The likelihood function given the data is:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p(y \mid X, \beta, \sigma^2) = \mathcal{N}(y \mid X\beta, \sigma^2 I)
</div>
<script type="math/tex; mode=display">
p(y \mid X, \beta, \sigma^2) = \mathcal{N}(y \mid X\beta, \sigma^2 I)
</script>
</div>
<p>This is a Gaussian distribution with mean <span class="arithmatex"><span class="MathJax_Preview">X\beta</span><script type="math/tex">X\beta</script></span> and covariance <span class="arithmatex"><span class="MathJax_Preview">\sigma^2 I</span><script type="math/tex">\sigma^2 I</script></span>.</p>
</li>
<li>
<p><strong>Posterior</strong> (Bayes' Theorem):</p>
<ul>
<li>Using Bayes' Theorem, the posterior distribution of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> given the data is:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p(\beta \mid X, y, \sigma^2) \propto p(y \mid X, \beta, \sigma^2) p(\beta)
</div>
<script type="math/tex; mode=display">
p(\beta \mid X, y, \sigma^2) \propto p(y \mid X, \beta, \sigma^2) p(\beta)
</script>
</div>
</li>
<li>
<p><strong>Maximum Likelihood Estimation</strong>:</p>
<ul>
<li>
<p>To find the <strong>maximum likelihood estimates</strong> (MLE) of the parameters <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>, we can maximize the likelihood function:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathcal{L}(\beta, \sigma^2 \mid X, y) = p(y \mid X, \beta, \sigma^2)
</div>
<script type="math/tex; mode=display">
\mathcal{L}(\beta, \sigma^2 \mid X, y) = p(y \mid X, \beta, \sigma^2)
</script>
</div>
</li>
<li>
<p>The log-likelihood is:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \mathcal{L}(\beta, \sigma^2 \mid X, y) = -\frac{N}{2} \log (2\pi \sigma^2) - \frac{1}{2\sigma^2} \| y - X\beta \|^2
</div>
<script type="math/tex; mode=display">
\log \mathcal{L}(\beta, \sigma^2 \mid X, y) = -\frac{N}{2} \log (2\pi \sigma^2) - \frac{1}{2\sigma^2} \| y - X\beta \|^2
</script>
</div>
</li>
<li>
<p>To maximize this, take the derivative with respect to <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>, set them to zero, and solve for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>.</p>
</li>
</ul>
</li>
</ol>
<h5 id="mle-for-betabeta"><strong>MLE for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></strong></h5>
<ul>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> (ignoring <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>) is the ordinary least squares (OLS) solution:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta} = (X^T X)^{-1} X^T y
</div>
<script type="math/tex; mode=display">
\hat{\beta} = (X^T X)^{-1} X^T y
</script>
</div>
<h5 id="mle-for-sigma2sigma2"><strong>MLE for <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span></strong></h5>
<ul>
<li>After finding <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}</span><script type="math/tex">\hat{\beta}</script></span>, the MLE of <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span> is:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\sigma}^2 = \frac{1}{N} \| y - X\hat{\beta} \|^2
</div>
<script type="math/tex; mode=display">
\hat{\sigma}^2 = \frac{1}{N} \| y - X\hat{\beta} \|^2
</script>
</div>
<h5 id="computational-techniques"><strong>Computational Techniques:</strong></h5>
<ul>
<li>
<p><strong>Analytical Approach:</strong> In most cases (like when the prior and likelihood are Gaussian), the posterior distribution is also Gaussian, so you can compute it analytically. This is the standard approach for simple Bayesian linear regression.</p>
</li>
<li>
<p><strong>Markov Chain Monte Carlo (MCMC):</strong> For more complex models (e.g., non-linear regression or non-Gaussian priors), you can use MCMC methods (such as <strong>Metropolis-Hastings</strong> or <strong>Gibbs sampling</strong>) to sample from the posterior distribution of the parameters.</p>
</li>
<li>
<p><strong>Iterative Methods (Variational Inference):</strong> For more computationally complex models, you might use iterative methods like <strong>Variational Inference</strong> or <strong>Expectation-Maximization (EM)</strong> to approximate the posterior.</p>
</li>
</ul>
<hr />
<h4 id="gaussian-processes"><strong>Gaussian Processes</strong></h4>
<p>A <strong>Gaussian Process</strong> (GP) is a <strong>non-parametric</strong> model used for regression tasks. Unlike typical regression models that assume a functional form (like linear regression), GPR assumes that the data is drawn from a <strong>Gaussian distribution</strong> over functions.</p>
<blockquote>
<p>[! Definition] Definition<br />
A <strong>Gaussian process</strong> is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by a <strong>mean function</strong> <span class="arithmatex"><span class="MathJax_Preview">m(x)</span><script type="math/tex">m(x)</script></span> and a <strong>covariance function</strong> (or kernel) <span class="arithmatex"><span class="MathJax_Preview">k(x, x')</span><script type="math/tex">k(x, x')</script></span>, which defines the correlation between inputs.</p>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
</div>
<script type="math/tex; mode=display">
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
</script>
</div>
<p>Where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> is the function value at input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">m(x)</span><script type="math/tex">m(x)</script></span> is the <strong>mean function</strong> (often assumed to be zero),</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">k(x, x')</span><script type="math/tex">k(x, x')</script></span> is the <strong>covariance function</strong>, which defines the relationship between points <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x'</span><script type="math/tex">x'</script></span>.</p>
</li>
<li>
<p><strong>Model Assumptions</strong>:</p>
<ul>
<li>
<p>Assume the output <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is a realization of a <strong>Gaussian process</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
f(x) \sim \mathcal{GP}(0, k(x, x')) 
</div>
<script type="math/tex; mode=display">  
f(x) \sim \mathcal{GP}(0, k(x, x')) 
</script>
</div>
</li>
</ul>
<p>where <span class="arithmatex"><span class="MathJax_Preview">k(x, x')</span><script type="math/tex">k(x, x')</script></span> is a <strong>covariance kernel</strong> that defines the relationship between the points <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x'</span><script type="math/tex">x'</script></span>.</p>
</li>
<li>
<p><strong>Likelihood</strong>:</p>
<ul>
<li>
<p>Given <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> training points <span class="arithmatex"><span class="MathJax_Preview">X = \{x_1, x_2, …, x_N\}</span><script type="math/tex">X = \{x_1, x_2, …, x_N\}</script></span> and corresponding outputs <span class="arithmatex"><span class="MathJax_Preview">y = \{y_1, y_2, …, y_N\}</span><script type="math/tex">y = \{y_1, y_2, …, y_N\}</script></span>, the likelihood of the observations is given by the <strong>multivariate normal distribution</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
</div>
<script type="math/tex; mode=display">  
p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
</script>
</div>
<p>where:</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">K(X, X)</span><script type="math/tex">K(X, X)</script></span> is the <strong>covariance matrix</strong> computed using the kernel function <span class="arithmatex"><span class="MathJax_Preview">k(x, x')</span><script type="math/tex">k(x, x')</script></span>,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span> is the noise variance, and</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">I</span><script type="math/tex">I</script></span> is the identity matrix.</p>
</li>
</ul>
</li>
<li>
<p><strong>Maximizing the Likelihood</strong>:</p>
<ul>
<li>
<p>The log-likelihood is:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
\log p(y \mid X, \theta) = -\frac{1}{2} y^T (K(X, X) + \sigma^2 I)^{-1} y - \frac{1}{2} \log \det (K(X, X) + \sigma^2 I) - \frac{N}{2} \log 2\pi    
</div>
<script type="math/tex; mode=display">  
\log p(y \mid X, \theta) = -\frac{1}{2} y^T (K(X, X) + \sigma^2 I)^{-1} y - \frac{1}{2} \log \det (K(X, X) + \sigma^2 I) - \frac{N}{2} \log 2\pi    
</script>
</div>
</li>
<li>
<p>Here, <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> refers to the kernel hyperparameters (e.g., length scale, variance) and <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span> is the noise parameter.</p>
</li>
</ul>
</li>
<li>
<p><strong>Optimization</strong>:</p>
<ul>
<li>
<p>To find the <strong>maximum likelihood estimate (MLE)</strong> of the kernel parameters <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>, we maximize the log-likelihood:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
\hat{\theta}, \hat{\sigma^2} = \arg \max_\theta \log p(y \mid X, \theta)    
</div>
<script type="math/tex; mode=display">  
\hat{\theta}, \hat{\sigma^2} = \arg \max_\theta \log p(y \mid X, \theta)    
</script>
</div>
</li>
<li>
<p>This is typically done via numerical optimization techniques like gradient descent or conjugate gradient methods, since the log-likelihood is non-linear with respect to the kernel parameters.</p>
</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:</p>
<ul>
<li>
<p>Once the kernel parameters are optimized, predictions at a new test point <span class="arithmatex"><span class="MathJax_Preview">x_*</span><script type="math/tex">x_*</script></span> can be made by conditioning the Gaussian process on the training data. The predictive mean and variance are given by:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mu_* = k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} y
</div>
<script type="math/tex; mode=display">
\mu_* = k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} y
</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">
\sigma_*^2 = k(x_*, x_*) - k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} k(X, x_*)
</div>
<script type="math/tex; mode=display">
\sigma_*^2 = k(x_*, x_*) - k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} k(X, x_*)
</script>
</div>
</li>
<li>
<p>Here, <span class="arithmatex"><span class="MathJax_Preview">k(x_*, X)</span><script type="math/tex">k(x_*, X)</script></span> is the covariance between the test point <span class="arithmatex"><span class="MathJax_Preview">x_*</span><script type="math/tex">x_*</script></span> and the training points <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">k(x_*, x_*)</span><script type="math/tex">k(x_*, x_*)</script></span> is the covariance at the test point.</p>
</li>
</ul>
</li>
<li>
<p><strong>Model Definition:</strong></p>
<p>A Gaussian Process defines a <strong>prior</strong> over functions. We assume the function values <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> at any set of points <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> follow a multivariate normal distribution:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
f(X) \sim \mathcal{N}(0, K(X, X))
</div>
<script type="math/tex; mode=display">  
f(X) \sim \mathcal{N}(0, K(X, X))
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">K(X, X)</span><script type="math/tex">K(X, X)</script></span> is the <strong>covariance matrix</strong> determined by the chosen kernel (covariance function). Common kernels include the <strong>RBF kernel</strong> (Radial Basis Function), <strong>Matern kernel</strong>, etc.</p>
</li>
<li>
<p><strong>Likelihood Function:</strong></p>
<p>Given the observations <span class="arithmatex"><span class="MathJax_Preview">y = f(X) + \epsilon</span><script type="math/tex">y = f(X) + \epsilon</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> is noise, the likelihood function is:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
</div>
<script type="math/tex; mode=display">  
p(y \mid X, \theta) = \mathcal{N}(y \mid 0, K(X, X) + \sigma^2 I)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span> is the noise variance.</p>
</li>
<li>
<p><strong>Posterior Distribution:</strong></p>
<p>The posterior distribution of the function values <span class="arithmatex"><span class="MathJax_Preview">f_*</span><script type="math/tex">f_*</script></span> at new test points <span class="arithmatex"><span class="MathJax_Preview">X_*</span><script type="math/tex">X_*</script></span>, given the training data <span class="arithmatex"><span class="MathJax_Preview">(X, y)</span><script type="math/tex">(X, y)</script></span>, is also Gaussian:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
p(f_* \mid X_*, X, y) = \mathcal{N}(f_* \mid \mu_*, \Sigma_*)   
</div>
<script type="math/tex; mode=display">  
p(f_* \mid X_*, X, y) = \mathcal{N}(f_* \mid \mu_*, \Sigma_*)   
</script>
</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\mu_*</span><script type="math/tex">\mu_*</script></span> is the mean of the posterior, and</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\Sigma_*</span><script type="math/tex">\Sigma_*</script></span> is the covariance (uncertainty).</p>
</li>
</ul>
<p>The mean and covariance of the posterior are given by:</p>
<div class="arithmatex">
<div class="MathJax_Preview">  
\mu_* = K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} y  
</div>
<script type="math/tex; mode=display">  
\mu_* = K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} y  
</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">  
\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} K(X, X_*) 
</div>
<script type="math/tex; mode=display">  
\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma^2 I]^{-1} K(X, X_*) 
</script>
</div>
</li>
<li>
<p><strong>Iterative Methods:</strong></p>
<ul>
<li>In practice, <strong>iterative optimization</strong> (e.g., <strong>gradient descent</strong> or <strong>conjugate gradient methods</strong>) is used to find the optimal kernel hyperparameters by maximizing the log marginal likelihood.</li>
</ul>
</li>
</ul>
<h3 id="j-optimization-algorithms-improving-model-training"><strong>J. Optimization Algorithms (Improving Model Training)</strong></h3>
<ul>
<li>
<p><strong>Gradient Descent Variants</strong>:</p>
<ul>
<li>
<p><strong>SGD (Stochastic Gradient Descent)</strong></p>
</li>
<li>
<p><strong>Momentum-based GD</strong></p>
</li>
<li>
<p><strong>Adam, RMSprop, Adagrad</strong></p>
</li>
</ul>
</li>
<li>
<p><strong>Second-order Optimization</strong>: Newton's method, Quasi-Newton methods (L-BFGS).</p>
</li>
<li>
<p><strong>Adaptive Learning Rate</strong>: Learning rate schedules (cosine annealing, warm restarts).</p>
</li>
</ul>
<h4 id="newtons-method"><strong>Newton's Method</strong></h4>
<p>Newton's method uses the <strong>Hessian matrix</strong> (second derivative) for optimization:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\theta_{t+1} = \theta_t - H^{-1} \nabla f(\theta)
</div>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - H^{-1} \nabla f(\theta)
</script>
</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> is the Hessian matrix (second derivative of the loss function).</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\nabla f(\theta)</span><script type="math/tex">\nabla f(\theta)</script></span> is the gradient.</p>
</li>
</ul>
<h4 id="momentum-based-update"><strong>Momentum-based Update</strong></h4>
<p>Momentum accelerates gradient descent by adding a fraction of the previous update to the current one:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
v_t = \beta v_{t-1} - \alpha \nabla f(\theta_t)
</div>
<script type="math/tex; mode=display">
v_t = \beta v_{t-1} - \alpha \nabla f(\theta_t)
</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">
\theta_{t+1} = \theta_t + v_t
</div>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t + v_t
</script>
</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">v_t</span><script type="math/tex">v_t</script></span> is the velocity term,</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> is the momentum coefficient (e.g., 0.9),</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is the learning rate.
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Python</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-14-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-14-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-14-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-14-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-14-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-14-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-14-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-14-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-14-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-14-10">10</a></span>
<span class="normal"><a href="#__codelineno-14-11">11</a></span>
<span class="normal"><a href="#__codelineno-14-12">12</a></span>
<span class="normal"><a href="#__codelineno-14-13">13</a></span>
<span class="normal"><a href="#__codelineno-14-14">14</a></span>
<span class="normal"><a href="#__codelineno-14-15">15</a></span>
<span class="normal"><a href="#__codelineno-14-16">16</a></span>
<span class="normal"><a href="#__codelineno-14-17">17</a></span>
<span class="normal"><a href="#__codelineno-14-18">18</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1"></a><span class="n">iterations</span> <span class="o">=</span> <span class="mi">500</span>
<a id="__codelineno-14-2" name="__codelineno-14-2"></a><span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>
<a id="__codelineno-14-3" name="__codelineno-14-3"></a><span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># Momentum coefficient</span>
<a id="__codelineno-14-4" name="__codelineno-14-4"></a><span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<a id="__codelineno-14-5" name="__codelineno-14-5"></a>
<a id="__codelineno-14-6" name="__codelineno-14-6"></a><span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
<a id="__codelineno-14-7" name="__codelineno-14-7"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">order</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span>
<a id="__codelineno-14-8" name="__codelineno-14-8"></a>    <span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>  <span class="c1"># Initialize velocity</span>
<a id="__codelineno-14-9" name="__codelineno-14-9"></a>
<a id="__codelineno-14-10" name="__codelineno-14-10"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
<a id="__codelineno-14-11" name="__codelineno-14-11"></a>        <span class="n">params</span> <span class="o">=</span> <span class="n">weights</span>
<a id="__codelineno-14-12" name="__codelineno-14-12"></a>        <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-14-13" name="__codelineno-14-13"></a>        <span class="n">params_grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-14-14" name="__codelineno-14-14"></a>
<a id="__codelineno-14-15" name="__codelineno-14-15"></a>        <span class="c1"># Update parameters using Momentum</span>
<a id="__codelineno-14-16" name="__codelineno-14-16"></a>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">):</span>
<a id="__codelineno-14-17" name="__codelineno-14-17"></a>            <span class="n">velocity</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">velocity</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">params_grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>  <span class="c1"># Apply momentum update</span>
<a id="__codelineno-14-18" name="__codelineno-14-18"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">velocity</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>  <span class="c1"># Update weights</span>
</code></pre></div></td></tr></table></div></p>
</li>
</ul>
<h3 id="k-model-architecture-optimization-deep-learning"><strong>K. Model Architecture Optimization (Deep Learning)</strong></h3>
<ul>
<li>
<p><strong>Residual Networks (ResNet)</strong>: Helps with vanishing gradients.</p>
</li>
<li>
<p><strong>Transformers</strong>: Attention-based models (BERT, GPT).</p>
</li>
<li>
<p><strong>Neural Architecture Search (NAS)</strong>: Auto-tuning model structure.</p>
</li>
</ul>
<h3 id="l-interpretability-explainability"><strong>L. Interpretability &amp; Explainability</strong></h3>
<ul>
<li>
<p><strong>SHAP, LIME</strong>: Explainable AI tools.</p>
</li>
<li>
<p><strong>Feature Importance Methods</strong>: Permutation importance, gradient-based saliency maps.</p>
</li>
</ul>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.annotate"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/math.js"></script>
      
    
  </body>
</html>